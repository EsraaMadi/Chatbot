{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Chatbot Model\n",
    "In this project, I'm going to build a neural network model using machine translation concept.  I will be training a sequence to sequence model on a dataset of movies conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from distutils.version import LooseVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Preprocessed Data and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them\n",
    "    \"\"\"\n",
    "    with open('models/preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the following:\n",
    "#     1. list of lists for the training data as index\n",
    "#     2. two sets of dict,a set for comments , another one for replay (used to convert words to indexes and vice versa)\n",
    "((source_int_text, target_int_text),\n",
    "(source_vocab_to_int, source_int_to_vocab),\n",
    "(target_vocab_to_int, target_int_to_vocab)) = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of comments: 13929\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size of comments:', len( source_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of replays: 13406\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size of replays:', len(target_vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) in [LooseVersion('1.10.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just tensor for saving\n",
    "def model_inputs():\n",
    "    '''\n",
    "    Function:\n",
    "        *return:\n",
    "            inputs: tensor of encode train input\n",
    "            targets: tensor of  decode train input\n",
    "            lr_rate: tensor of learning rate\n",
    "            keep_prob: tensor of the keep probability for Dropouts\n",
    "            target_sequence_length: tensor of length of each sentance in targets tensor\n",
    "            max_target_length: max of previous tensor\n",
    "            source_sequence_length: tensor of length of each sentance in inputs tensor\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32,shape=[None,None],name='input') # [batch size,sentences lengths] Q\n",
    "    targets = tf.placeholder(tf.int32,shape=[None,None],name='targets') # [batch size,sentences lengths] A\n",
    "    \n",
    "    learningrate = tf.placeholder(tf.float32,shape=[],name='learningrate')\n",
    "    keep_prob = tf.placeholder(tf.float32,shape=[],name='keep_prob')\n",
    "    \n",
    "    #  The maximum length of setence is different from batch to batch, so it cannot be set with the exact number\n",
    "    # This particular value is required as an argument of TrainerHelper to build decoder model for training.\n",
    "    target_sequence_length = tf.placeholder(tf.int32,[None,],name='target_sequence_length')\n",
    "    \n",
    "    #gets the maximum value out of lengths of all the target sentences(sequences)\n",
    "    max_target_length = tf.reduce_max(target_sequence_length)\n",
    "    \n",
    "    source_sequence_length = tf.placeholder(tf.int32,[None,],name='source_sequence_length')\n",
    "    \n",
    "    return (inputs, targets, learningrate, keep_prob, target_sequence_length, max_target_length, source_sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "1. Remove the last word id from each batch in target_data\n",
    "2. concat the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Function to preprocess target data for encoding\n",
    "        *args:\n",
    "            target_data: tensor, current batch (replay) (Target Placehoder)\n",
    "            vocab_to_int: ictionary to go from the target words to an id\n",
    "            batch_size: number \n",
    "        *return:\n",
    "            after_concat:Preprocessed target data after:\n",
    "                            1- slice data to batches size to remove the last word id from each batch\n",
    "                            2- add '<GO>' add to each sentance at the begining \n",
    "    \"\"\"\n",
    "        \n",
    "    #  splitting into multiple tensors with the striding window size from begin to end\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1,1])  # TF Tensor, Begin, End, Strides\n",
    "    \n",
    "    # fill : creates a tensor filled with a scalar value. args: TF Tensor, value to fill\n",
    "    # concat : concatenates tensors along one dimension. tensor['<GO'] + ending\n",
    "    decoder_input = tf.concat([tf.fill([batch_size,1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell(rnn_size):\n",
    "    '''\n",
    "    Function to create LSTM layer\n",
    "        *args:\n",
    "            rnn_size: number , how many hidden units we have in each layer\n",
    "        *return: LSTM layer\n",
    "    '''\n",
    "    # LSTMCell: simply specifies how many internal units it has\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "    \n",
    "    # DropoutWrapper: wraps a cell with keep probability value\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, source_sequence_length, source_vocab_size,encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Function: Build encoding model that consists of two different parts.\n",
    "        1- the embedding layer. Each word in a sentence will be represented with the number of features specified as embedding_size.\n",
    "        2- the RNN layer(s).\n",
    "        *args:\n",
    "            rnn_inputs: tensor, current batch (comments), reversed\n",
    "            rnn_size: number , how many hidden units we have in each layer\n",
    "            num_layers: number , how many hidden layers\n",
    "            keep_prob: Dropout keep probability\n",
    "            source_sequence_length : length \n",
    "            source_vocab_size : number, vocablary size\n",
    "            encoding_embedding_size: number , dimenations\n",
    "        *return:\n",
    "            RNN output:\n",
    "            RNN state:\n",
    "    \"\"\"\n",
    "    # Each word in a sentence will be represented with the number of features specified as encoding_embedding_size\n",
    "    enc_embed = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size, encoding_embedding_size)\n",
    "    \n",
    "    # MultiRNNCell: stacks multiple RNN (type) cells\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    # dynamic_rnn: put Embedding layer and RNN layer(s) all together\n",
    "    encoding_output, encoding_state = tf.nn.dynamic_rnn(enc_cell, enc_embed, \n",
    "                                                        sequence_length=source_sequence_length,dtype=tf.float32)\n",
    "    \n",
    "    return encoding_output, encoding_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Model\n",
    "Decoding model can be thought of two separate processes, training and inference. They share the same parameters but they have different strategy to feed the shared model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    '''\n",
    "    Function to Create a training process in decoding layer \n",
    "        *args:\n",
    "            encoder_state: encoder long term momery LTM\n",
    "            dec_cell: created MultiRNN Cells \n",
    "            dec_embed_input: embedded current batch (replay) \n",
    "            target_sequence_length: list of lengths for each sentence in current batch (replay)\n",
    "            max_summary_length: number, The length of the longest sequence in the batch (replay)\n",
    "            output_layer: created output layer\n",
    "            keep_prob: Dropout keep probability\n",
    "        *return:\n",
    "            training_decoder_output: BasicDecoderOutput containing training logits and sample_id\n",
    "    '''\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=target_sequence_length,\n",
    "                                                        time_major=False)\n",
    "    \n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       encoder_state,\n",
    "                                                       output_layer)\n",
    "    \n",
    "    training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                impute_finished=True,\n",
    "                                                                maximum_iterations=max_summary_length)[0]\n",
    "    return training_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    '''\n",
    "    Function Create a inference process in decoding layer  \n",
    "        *args:\n",
    "            encoder_state: encoder long term momery LTM\n",
    "            dec_cell: created MultiRNN Cells \n",
    "            dec_embeddings: Weights for embedding\n",
    "            start_of_sequence_id: index of '<GO>' in vocab_to_int dict \n",
    "            end_of_sequence_id: index of '<EOS>' in vocab_to_int dict\n",
    "            max_target_sequence_length: number, The length of the longest sequence in the batch (replay)\n",
    "            vocab_size: number\n",
    "            output_layer: created output layer\n",
    "            batch_size: number\n",
    "            keep_prob: Dropout keep probability\n",
    "        *return:\n",
    "            inference_decoder_output: BasicDecoderOutput containing inference logits and sample_id\n",
    "    '''\n",
    "\n",
    "    start_tokens = tf.tile(tf.constant([start_of_sequence_id], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_of_sequence_id)\n",
    "    \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        encoder_state,\n",
    "                                                        output_layer)\n",
    "    \n",
    "    inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
    "    return inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state, target_sequence_length,\n",
    "                   max_target_sequence_length, rnn_size, num_layers,\n",
    "                   target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    '''\n",
    "    Function to Create decoding layer\n",
    "        *args:\n",
    "            dec_input: tensor of tensor, current batch (replay) after apply process_decoder_input function (add '<GO>')\n",
    "            encoder_state: encoder long term momery LTM\n",
    "            target_sequence_length: list of lengths for each sentence in current batch (replay)\n",
    "            max_target_sequence_length: number, longest length of (relay) sentence in the whole file\n",
    "            rnn_size: number , how many hidden units we have in each layed\n",
    "            num_layers: number , how many hidden layers\n",
    "            target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "            target_vocab_size: Size of target vocabulary\n",
    "            batch_size: number\n",
    "            keep_prob: keep_probability of droupout\n",
    "            decoding_embedding_size: Decoding embedding size\n",
    "        *return:\n",
    "            train_output: Training BasicDecoderOutput\n",
    "            infer_output: Inference BasicDecoderOutput\n",
    "    '''\n",
    "    # 1. Decoder Embedding\n",
    "    '''\n",
    "     TF nn.embedding_lookup + manually created embedding parameters returns the similar result to the TF contrib.\n",
    "     layers.embed_sequence. For the inference process,whenever the output of the current time step is calculated via decoder, \n",
    "     it will be embeded by the shared embedding parameter and become the input for the next time step.\n",
    "     You only need to provide the embedding parameter to the GreedyEmbeddingHelper\n",
    "    '''\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "    # decoder - training\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        #Create an output layer to map the outputs of the decoder to the elements of our vocabulary\n",
    "        # This is just a fully connected layer to get probabilities of occurance of each words at the end\n",
    "        train_decoder_out = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n",
    "    \n",
    "    # decode - inference\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_decoder_out = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, \n",
    "                             target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, \n",
    "                             target_vocab_size, output_layer, batch_size, keep_prob)\n",
    "        \n",
    "    return (train_decoder_out, infer_decoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    '''\n",
    "    Function to build the Sequence-to-Sequence model\n",
    "        *args:\n",
    "            input_data: tensor, current batch (comments), reversed  placeholder\n",
    "            target_data: tensor, current batch (replay) placeholder\n",
    "            keep_prob: Dropout keep probability placeholder\n",
    "            batch_size: number\n",
    "            source_sequence_length: list of lengths for each sentence in current batch (comment)\n",
    "            target_sequence_length: list of lengths for each sentence in current batch (replay)\n",
    "            max_target_sentence_length: number, longest length of (relay) sentence in the whole file\n",
    "            source_vocab_size : number, Source vocablary size\n",
    "            target_vocab_size : number, Target vocablary size\n",
    "            embedding_size: number , dimenations\n",
    "            enc_embedding_size: number, Decoder embedding size\n",
    "            dec_embedding_size: number, Encoder embedding size\n",
    "            rnn_size: number , how many hidden units we have in each layed\n",
    "            num_layers: number , how many hidden layers\n",
    "            vocab_to_int: target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "        *return:\n",
    "            training_decoder_output: Training BasicDecoderOutput\n",
    "            inference_decoder_output: Inference BasicDecoderOutput\n",
    "    '''\n",
    "    \n",
    "    _, enc_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(dec_input, enc_state,\n",
    "                   target_sequence_length, max_target_sentence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 256\n",
    "decoding_embedding_size = 256\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.75\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "((source_int_text, target_int_text),\n",
    "(source_vocab_to_int, source_int_to_vocab),\n",
    "(target_vocab_to_int, target_int_to_vocab)) = load_preprocess()\n",
    "\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Graph contains a set of tf.Operation objects, which represent units of computation;\n",
    "# overrides the current default graph for the lifetime \n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # encode inputs\n",
    "    # decode inputs \n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    #Return a tensor with the same shape and contents as input\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    '''creates [batch_size, max_target_sequence_length] size of variable, \n",
    "    then maks only the first target_sequence_length number of elements to 1.\n",
    "    It means parts will have less weight than others.'''\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    #used to group some variables together in an op\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch and pad the source and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    ''' Function to pad sentences with <PAD> so that each sentence of a batch has the same length\n",
    "            *args:\n",
    "               sentence_batch: list of lists\n",
    "               pad_int: '<pad>' code\n",
    "            *return:\n",
    "               list of lists : resize each sentance in the batch by adding '<pad>' at the end\n",
    "    '''\n",
    "    # get the length of longest sentence in the batch\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    \n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    '''\n",
    "    Function to batch targets, sources, and the lengths of their sentences together\n",
    "        *args:\n",
    "            sources: list of lists for each sentance as index not words (comment)\n",
    "            targets: list of lists for each sentance as index not words (replay)\n",
    "            batch_size: number , how many sentance will process each time \n",
    "            source_pad_int: index of '<pad>' in source vocab dict\n",
    "            target_pad_int: index of '<pad>' in target vocab dict\n",
    "        * return:\n",
    "            pad_sources_batch: np array for each batch after padding the end (comment)\n",
    "            pad_targets_batch: np array for each batch after padding the end (replay)\n",
    "            pad_source_lengths: list, length of each senetnce in the batch \n",
    "            pad_targets_lengths: list, length of each senetnce in the batch \n",
    "    '''\n",
    "    \n",
    "    # for each batch:\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "Train the neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   10/355 - Train Accuracy: 0.7112, Validation Accuracy: 0.7306, Loss: 2.0971\n",
      "Epoch   0 Batch   20/355 - Train Accuracy: 0.1532, Validation Accuracy: 0.1328, Loss: 1.8223\n",
      "Epoch   0 Batch   30/355 - Train Accuracy: 0.5167, Validation Accuracy: 0.5251, Loss: 1.5579\n",
      "Epoch   0 Batch   40/355 - Train Accuracy: 0.6510, Validation Accuracy: 0.6618, Loss: 1.4929\n",
      "Epoch   0 Batch   50/355 - Train Accuracy: 0.7016, Validation Accuracy: 0.7219, Loss: 1.6353\n",
      "Epoch   0 Batch   60/355 - Train Accuracy: 0.6633, Validation Accuracy: 0.7136, Loss: 1.6643\n",
      "Epoch   0 Batch   70/355 - Train Accuracy: 0.6766, Validation Accuracy: 0.6994, Loss: 1.5168\n",
      "Epoch   0 Batch   80/355 - Train Accuracy: 0.7136, Validation Accuracy: 0.7252, Loss: 1.4154\n",
      "Epoch   0 Batch   90/355 - Train Accuracy: 0.6790, Validation Accuracy: 0.7136, Loss: 1.4779\n",
      "Epoch   0 Batch  100/355 - Train Accuracy: 0.6812, Validation Accuracy: 0.6836, Loss: 1.3092\n",
      "Epoch   0 Batch  110/355 - Train Accuracy: 0.7444, Validation Accuracy: 0.7252, Loss: 1.0682\n",
      "Epoch   0 Batch  120/355 - Train Accuracy: 0.6973, Validation Accuracy: 0.7144, Loss: 1.3061\n",
      "Epoch   0 Batch  130/355 - Train Accuracy: 0.7973, Validation Accuracy: 0.7144, Loss: 0.9120\n",
      "Epoch   0 Batch  140/355 - Train Accuracy: 0.6255, Validation Accuracy: 0.6994, Loss: 1.8458\n",
      "Epoch   0 Batch  150/355 - Train Accuracy: 0.7015, Validation Accuracy: 0.7001, Loss: 1.3041\n",
      "Epoch   0 Batch  160/355 - Train Accuracy: 0.7219, Validation Accuracy: 0.7252, Loss: 1.2430\n",
      "Epoch   0 Batch  170/355 - Train Accuracy: 0.6839, Validation Accuracy: 0.7144, Loss: 1.5136\n",
      "Epoch   0 Batch  180/355 - Train Accuracy: 0.6747, Validation Accuracy: 0.7252, Loss: 1.5813\n",
      "Epoch   0 Batch  190/355 - Train Accuracy: 0.6327, Validation Accuracy: 0.6618, Loss: 1.4315\n",
      "Epoch   0 Batch  200/355 - Train Accuracy: 0.6789, Validation Accuracy: 0.7267, Loss: 1.5307\n",
      "Epoch   0 Batch  210/355 - Train Accuracy: 0.7164, Validation Accuracy: 0.7267, Loss: 1.3790\n",
      "Epoch   0 Batch  220/355 - Train Accuracy: 0.6190, Validation Accuracy: 0.6618, Loss: 1.7491\n",
      "Epoch   0 Batch  230/355 - Train Accuracy: 0.6154, Validation Accuracy: 0.5862, Loss: 1.1678\n",
      "Epoch   0 Batch  240/355 - Train Accuracy: 0.6825, Validation Accuracy: 0.7144, Loss: 1.5316\n",
      "Epoch   0 Batch  250/355 - Train Accuracy: 0.7172, Validation Accuracy: 0.7267, Loss: 1.2489\n",
      "Epoch   0 Batch  260/355 - Train Accuracy: 0.7064, Validation Accuracy: 0.7219, Loss: 1.3140\n",
      "Epoch   0 Batch  270/355 - Train Accuracy: 0.6783, Validation Accuracy: 0.7252, Loss: 1.5008\n",
      "Epoch   0 Batch  280/355 - Train Accuracy: 0.6952, Validation Accuracy: 0.7136, Loss: 1.3831\n",
      "Epoch   0 Batch  290/355 - Train Accuracy: 0.2742, Validation Accuracy: 0.2825, Loss: 1.4380\n",
      "Epoch   0 Batch  300/355 - Train Accuracy: 0.7170, Validation Accuracy: 0.7267, Loss: 1.2866\n",
      "Epoch   0 Batch  310/355 - Train Accuracy: 0.7104, Validation Accuracy: 0.7252, Loss: 1.2290\n",
      "Epoch   0 Batch  320/355 - Train Accuracy: 0.6541, Validation Accuracy: 0.6994, Loss: 1.5420\n",
      "Epoch   0 Batch  330/355 - Train Accuracy: 0.7188, Validation Accuracy: 0.7267, Loss: 1.2985\n",
      "Epoch   0 Batch  340/355 - Train Accuracy: 0.7211, Validation Accuracy: 0.7252, Loss: 1.1811\n",
      "Epoch   0 Batch  350/355 - Train Accuracy: 0.7128, Validation Accuracy: 0.7267, Loss: 1.2468\n",
      "Model Trained and Saved\n",
      "Epoch   1 Batch   10/355 - Train Accuracy: 0.7037, Validation Accuracy: 0.7219, Loss: 1.2656\n",
      "Epoch   1 Batch   20/355 - Train Accuracy: 0.3684, Validation Accuracy: 0.3559, Loss: 1.4177\n",
      "Epoch   1 Batch   30/355 - Train Accuracy: 0.7144, Validation Accuracy: 0.7252, Loss: 1.2634\n",
      "Epoch   1 Batch   40/355 - Train Accuracy: 0.7091, Validation Accuracy: 0.7252, Loss: 1.2594\n",
      "Epoch   1 Batch   50/355 - Train Accuracy: 0.7016, Validation Accuracy: 0.7219, Loss: 1.4385\n",
      "Epoch   1 Batch   60/355 - Train Accuracy: 0.6189, Validation Accuracy: 0.6618, Loss: 1.4404\n",
      "Epoch   1 Batch   70/355 - Train Accuracy: 0.6184, Validation Accuracy: 0.6376, Loss: 1.3492\n",
      "Epoch   1 Batch   80/355 - Train Accuracy: 0.7136, Validation Accuracy: 0.7252, Loss: 1.2830\n",
      "Epoch   1 Batch   90/355 - Train Accuracy: 0.1903, Validation Accuracy: 0.2084, Loss: 1.3344\n",
      "Epoch   1 Batch  100/355 - Train Accuracy: 0.7178, Validation Accuracy: 0.7252, Loss: 1.1863\n",
      "Epoch   1 Batch  110/355 - Train Accuracy: 0.7506, Validation Accuracy: 0.7219, Loss: 0.9693\n",
      "Epoch   1 Batch  120/355 - Train Accuracy: 0.7048, Validation Accuracy: 0.7252, Loss: 1.1825\n",
      "Epoch   1 Batch  130/355 - Train Accuracy: 0.7973, Validation Accuracy: 0.7144, Loss: 0.8382\n",
      "Epoch   1 Batch  140/355 - Train Accuracy: 0.1938, Validation Accuracy: 0.1699, Loss: 1.6772\n",
      "Epoch   1 Batch  150/355 - Train Accuracy: 0.7231, Validation Accuracy: 0.7252, Loss: 1.1958\n",
      "Epoch   1 Batch  160/355 - Train Accuracy: 0.4356, Validation Accuracy: 0.4585, Loss: 1.1382\n",
      "Epoch   1 Batch  170/355 - Train Accuracy: 0.6917, Validation Accuracy: 0.7252, Loss: 1.3913\n",
      "Epoch   1 Batch  180/355 - Train Accuracy: 0.6747, Validation Accuracy: 0.7252, Loss: 1.4444\n",
      "Epoch   1 Batch  190/355 - Train Accuracy: 0.3887, Validation Accuracy: 0.3912, Loss: 1.2939\n",
      "Epoch   1 Batch  200/355 - Train Accuracy: 0.6773, Validation Accuracy: 0.7252, Loss: 1.3857\n",
      "Epoch   1 Batch  210/355 - Train Accuracy: 0.7164, Validation Accuracy: 0.7267, Loss: 1.2586\n",
      "Epoch   1 Batch  220/355 - Train Accuracy: 0.6125, Validation Accuracy: 0.6499, Loss: 1.6124\n",
      "Epoch   1 Batch  230/355 - Train Accuracy: 0.7395, Validation Accuracy: 0.7267, Loss: 1.0583\n",
      "Epoch   1 Batch  240/355 - Train Accuracy: 0.6825, Validation Accuracy: 0.7144, Loss: 1.3965\n",
      "Epoch   1 Batch  250/355 - Train Accuracy: 0.7013, Validation Accuracy: 0.7136, Loss: 1.1390\n",
      "Epoch   1 Batch  260/355 - Train Accuracy: 0.7075, Validation Accuracy: 0.7267, Loss: 1.2088\n",
      "Epoch   1 Batch  270/355 - Train Accuracy: 0.6783, Validation Accuracy: 0.7252, Loss: 1.3807\n",
      "Epoch   1 Batch  280/355 - Train Accuracy: 0.7073, Validation Accuracy: 0.7252, Loss: 1.2884\n",
      "Epoch   1 Batch  290/355 - Train Accuracy: 0.6995, Validation Accuracy: 0.7252, Loss: 1.3315\n",
      "Epoch   1 Batch  300/355 - Train Accuracy: 0.7175, Validation Accuracy: 0.7252, Loss: 1.1937\n",
      "Epoch   1 Batch  310/355 - Train Accuracy: 0.7015, Validation Accuracy: 0.7136, Loss: 1.1410\n",
      "Epoch   1 Batch  320/355 - Train Accuracy: 0.6543, Validation Accuracy: 0.7144, Loss: 1.4210\n",
      "Epoch   1 Batch  330/355 - Train Accuracy: 0.7138, Validation Accuracy: 0.7252, Loss: 1.2103\n",
      "Epoch   1 Batch  340/355 - Train Accuracy: 0.7170, Validation Accuracy: 0.7267, Loss: 1.1001\n",
      "Epoch   1 Batch  350/355 - Train Accuracy: 0.7128, Validation Accuracy: 0.7267, Loss: 1.1689\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch   10/355 - Train Accuracy: 0.7061, Validation Accuracy: 0.7267, Loss: 1.1743\n",
      "Epoch   2 Batch   20/355 - Train Accuracy: 0.6827, Validation Accuracy: 0.7136, Loss: 1.3415\n",
      "Epoch   2 Batch   30/355 - Train Accuracy: 0.7144, Validation Accuracy: 0.7252, Loss: 1.1853\n",
      "Epoch   2 Batch   40/355 - Train Accuracy: 0.7091, Validation Accuracy: 0.7252, Loss: 1.1905\n",
      "Epoch   2 Batch   50/355 - Train Accuracy: 0.7046, Validation Accuracy: 0.7252, Loss: 1.3722\n",
      "Epoch   2 Batch   60/355 - Train Accuracy: 0.6736, Validation Accuracy: 0.7267, Loss: 1.3651\n",
      "Epoch   2 Batch   70/355 - Train Accuracy: 0.6636, Validation Accuracy: 0.6836, Loss: 1.2871\n",
      "Epoch   2 Batch   80/355 - Train Accuracy: 0.7136, Validation Accuracy: 0.7252, Loss: 1.2295\n",
      "Epoch   2 Batch   90/355 - Train Accuracy: 0.1069, Validation Accuracy: 0.0942, Loss: 1.2684\n",
      "Epoch   2 Batch  100/355 - Train Accuracy: 0.7073, Validation Accuracy: 0.7144, Loss: 1.1182\n",
      "Epoch   2 Batch  110/355 - Train Accuracy: 0.7480, Validation Accuracy: 0.7267, Loss: 0.9260\n",
      "Epoch   2 Batch  120/355 - Train Accuracy: 0.7048, Validation Accuracy: 0.7252, Loss: 1.1317\n",
      "Epoch   2 Batch  130/355 - Train Accuracy: 0.7973, Validation Accuracy: 0.7144, Loss: 0.8066\n",
      "Epoch   2 Batch  140/355 - Train Accuracy: 0.3017, Validation Accuracy: 0.2816, Loss: 1.6023\n",
      "Epoch   2 Batch  150/355 - Train Accuracy: 0.7231, Validation Accuracy: 0.7252, Loss: 1.1418\n",
      "Epoch   2 Batch  160/355 - Train Accuracy: 0.6669, Validation Accuracy: 0.6834, Loss: 1.0866\n",
      "Epoch   2 Batch  170/355 - Train Accuracy: 0.6839, Validation Accuracy: 0.7144, Loss: 1.3339\n",
      "Epoch   2 Batch  180/355 - Train Accuracy: 0.6851, Validation Accuracy: 0.7218, Loss: 1.3809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch  190/355 - Train Accuracy: 0.6801, Validation Accuracy: 0.7252, Loss: 1.2360\n",
      "Epoch   2 Batch  200/355 - Train Accuracy: 0.6789, Validation Accuracy: 0.7267, Loss: 1.3287\n",
      "Epoch   2 Batch  210/355 - Train Accuracy: 0.7111, Validation Accuracy: 0.7252, Loss: 1.2028\n",
      "Epoch   2 Batch  220/355 - Train Accuracy: 0.6490, Validation Accuracy: 0.7267, Loss: 1.5573\n",
      "Epoch   2 Batch  230/355 - Train Accuracy: 0.7434, Validation Accuracy: 0.7252, Loss: 1.0210\n",
      "Epoch   2 Batch  240/355 - Train Accuracy: 0.6825, Validation Accuracy: 0.7144, Loss: 1.3361\n",
      "Epoch   2 Batch  250/355 - Train Accuracy: 0.7172, Validation Accuracy: 0.7267, Loss: 1.0853\n",
      "Epoch   2 Batch  260/355 - Train Accuracy: 0.7025, Validation Accuracy: 0.7252, Loss: 1.1689\n",
      "Epoch   2 Batch  270/355 - Train Accuracy: 0.6821, Validation Accuracy: 0.7267, Loss: 1.3412\n",
      "Epoch   2 Batch  280/355 - Train Accuracy: 0.7073, Validation Accuracy: 0.7252, Loss: 1.2463\n",
      "Epoch   2 Batch  290/355 - Train Accuracy: 0.6720, Validation Accuracy: 0.6994, Loss: 1.2854\n",
      "Epoch   2 Batch  300/355 - Train Accuracy: 0.7175, Validation Accuracy: 0.7252, Loss: 1.1552\n",
      "Epoch   2 Batch  310/355 - Train Accuracy: 0.6157, Validation Accuracy: 0.6125, Loss: 1.1030\n",
      "Epoch   2 Batch  320/355 - Train Accuracy: 0.6543, Validation Accuracy: 0.7144, Loss: 1.3648\n",
      "Epoch   2 Batch  330/355 - Train Accuracy: 0.7138, Validation Accuracy: 0.7252, Loss: 1.1645\n",
      "Epoch   2 Batch  340/355 - Train Accuracy: 0.7211, Validation Accuracy: 0.7252, Loss: 1.0601\n",
      "Epoch   2 Batch  350/355 - Train Accuracy: 0.7123, Validation Accuracy: 0.7267, Loss: 1.1271\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch   10/355 - Train Accuracy: 0.7066, Validation Accuracy: 0.7243, Loss: 1.1376\n",
      "Epoch   3 Batch   20/355 - Train Accuracy: 0.6827, Validation Accuracy: 0.7136, Loss: 1.3018\n",
      "Epoch   3 Batch   30/355 - Train Accuracy: 0.7144, Validation Accuracy: 0.7252, Loss: 1.1544\n",
      "Epoch   3 Batch   40/355 - Train Accuracy: 0.7091, Validation Accuracy: 0.7252, Loss: 1.1588\n",
      "Epoch   3 Batch   50/355 - Train Accuracy: 0.7046, Validation Accuracy: 0.7252, Loss: 1.3319\n",
      "Epoch   3 Batch   60/355 - Train Accuracy: 0.6736, Validation Accuracy: 0.7267, Loss: 1.3250\n",
      "Epoch   3 Batch   70/355 - Train Accuracy: 0.6766, Validation Accuracy: 0.6994, Loss: 1.2478\n",
      "Epoch   3 Batch   80/355 - Train Accuracy: 0.7136, Validation Accuracy: 0.7252, Loss: 1.2018\n",
      "Epoch   3 Batch   90/355 - Train Accuracy: 0.2279, Validation Accuracy: 0.2446, Loss: 1.2321\n",
      "Epoch   3 Batch  100/355 - Train Accuracy: 0.7073, Validation Accuracy: 0.7147, Loss: 1.0886\n",
      "Epoch   3 Batch  110/355 - Train Accuracy: 0.7480, Validation Accuracy: 0.7267, Loss: 0.8932\n",
      "Epoch   3 Batch  120/355 - Train Accuracy: 0.6973, Validation Accuracy: 0.7147, Loss: 1.1058\n",
      "Epoch   3 Batch  130/355 - Train Accuracy: 0.7973, Validation Accuracy: 0.7144, Loss: 0.7872\n",
      "Epoch   3 Batch  140/355 - Train Accuracy: 0.3012, Validation Accuracy: 0.2816, Loss: 1.5576\n",
      "Epoch   3 Batch  150/355 - Train Accuracy: 0.7231, Validation Accuracy: 0.7252, Loss: 1.1100\n",
      "Epoch   3 Batch  160/355 - Train Accuracy: 0.6667, Validation Accuracy: 0.6834, Loss: 1.0626\n",
      "Epoch   3 Batch  170/355 - Train Accuracy: 0.6848, Validation Accuracy: 0.7141, Loss: 1.3045\n",
      "Epoch   3 Batch  180/355 - Train Accuracy: 0.6851, Validation Accuracy: 0.7219, Loss: 1.3450\n",
      "Epoch   3 Batch  190/355 - Train Accuracy: 0.6800, Validation Accuracy: 0.7252, Loss: 1.2148\n",
      "Epoch   3 Batch  200/355 - Train Accuracy: 0.3934, Validation Accuracy: 0.4253, Loss: 1.2977\n",
      "Epoch   3 Batch  210/355 - Train Accuracy: 0.7111, Validation Accuracy: 0.7252, Loss: 1.1704\n",
      "Epoch   3 Batch  220/355 - Train Accuracy: 0.6486, Validation Accuracy: 0.7261, Loss: 1.5234\n",
      "Epoch   3 Batch  230/355 - Train Accuracy: 0.7434, Validation Accuracy: 0.7251, Loss: 1.0037\n",
      "Epoch   3 Batch  240/355 - Train Accuracy: 0.6825, Validation Accuracy: 0.7147, Loss: 1.3014\n",
      "Epoch   3 Batch  250/355 - Train Accuracy: 0.7170, Validation Accuracy: 0.7267, Loss: 1.0525\n",
      "Epoch   3 Batch  260/355 - Train Accuracy: 0.7027, Validation Accuracy: 0.7254, Loss: 1.1408\n",
      "Epoch   3 Batch  270/355 - Train Accuracy: 0.6824, Validation Accuracy: 0.7249, Loss: 1.3047\n",
      "Epoch   3 Batch  280/355 - Train Accuracy: 0.7087, Validation Accuracy: 0.7243, Loss: 1.2242\n",
      "Epoch   3 Batch  290/355 - Train Accuracy: 0.4120, Validation Accuracy: 0.4231, Loss: 1.2592\n",
      "Epoch   3 Batch  300/355 - Train Accuracy: 0.7175, Validation Accuracy: 0.7252, Loss: 1.1247\n",
      "Epoch   3 Batch  310/355 - Train Accuracy: 0.5513, Validation Accuracy: 0.5619, Loss: 1.0865\n",
      "Epoch   3 Batch  320/355 - Train Accuracy: 0.6543, Validation Accuracy: 0.7144, Loss: 1.3354\n",
      "Epoch   3 Batch  330/355 - Train Accuracy: 0.7159, Validation Accuracy: 0.7216, Loss: 1.1416\n",
      "Epoch   3 Batch  340/355 - Train Accuracy: 0.6902, Validation Accuracy: 0.7052, Loss: 1.0335\n",
      "Epoch   3 Batch  350/355 - Train Accuracy: 0.6847, Validation Accuracy: 0.6968, Loss: 1.0998\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch   10/355 - Train Accuracy: 0.7064, Validation Accuracy: 0.7236, Loss: 1.1171\n",
      "Epoch   4 Batch   20/355 - Train Accuracy: 0.6870, Validation Accuracy: 0.7181, Loss: 1.2864\n",
      "Epoch   4 Batch   30/355 - Train Accuracy: 0.7150, Validation Accuracy: 0.7252, Loss: 1.1348\n",
      "Epoch   4 Batch   40/355 - Train Accuracy: 0.7055, Validation Accuracy: 0.7225, Loss: 1.1334\n",
      "Epoch   4 Batch   50/355 - Train Accuracy: 0.7060, Validation Accuracy: 0.7240, Loss: 1.3044\n",
      "Epoch   4 Batch   60/355 - Train Accuracy: 0.6725, Validation Accuracy: 0.7239, Loss: 1.3002\n",
      "Epoch   4 Batch   70/355 - Train Accuracy: 0.6892, Validation Accuracy: 0.7112, Loss: 1.2287\n",
      "Epoch   4 Batch   80/355 - Train Accuracy: 0.7136, Validation Accuracy: 0.7252, Loss: 1.1707\n",
      "Epoch   4 Batch   90/355 - Train Accuracy: 0.2635, Validation Accuracy: 0.2820, Loss: 1.2051\n",
      "Epoch   4 Batch  100/355 - Train Accuracy: 0.7100, Validation Accuracy: 0.7162, Loss: 1.0659\n",
      "Epoch   4 Batch  110/355 - Train Accuracy: 0.7462, Validation Accuracy: 0.7191, Loss: 0.8826\n",
      "Epoch   4 Batch  120/355 - Train Accuracy: 0.6976, Validation Accuracy: 0.7150, Loss: 1.0844\n",
      "Epoch   4 Batch  130/355 - Train Accuracy: 0.7973, Validation Accuracy: 0.7144, Loss: 0.7736\n",
      "Epoch   4 Batch  140/355 - Train Accuracy: 0.3718, Validation Accuracy: 0.3571, Loss: 1.5335\n",
      "Epoch   4 Batch  150/355 - Train Accuracy: 0.7231, Validation Accuracy: 0.7252, Loss: 1.0859\n",
      "Epoch   4 Batch  160/355 - Train Accuracy: 0.7163, Validation Accuracy: 0.7218, Loss: 1.0397\n",
      "Epoch   4 Batch  170/355 - Train Accuracy: 0.6910, Validation Accuracy: 0.7236, Loss: 1.2779\n",
      "Epoch   4 Batch  180/355 - Train Accuracy: 0.6821, Validation Accuracy: 0.7231, Loss: 1.3191\n",
      "Epoch   4 Batch  190/355 - Train Accuracy: 0.4830, Validation Accuracy: 0.4892, Loss: 1.1944\n",
      "Epoch   4 Batch  200/355 - Train Accuracy: 0.2808, Validation Accuracy: 0.3152, Loss: 1.2722\n",
      "Epoch   4 Batch  210/355 - Train Accuracy: 0.7120, Validation Accuracy: 0.7243, Loss: 1.1479\n",
      "Epoch   4 Batch  220/355 - Train Accuracy: 0.6474, Validation Accuracy: 0.7252, Loss: 1.4954\n",
      "Epoch   4 Batch  230/355 - Train Accuracy: 0.7431, Validation Accuracy: 0.7251, Loss: 0.9866\n",
      "Epoch   4 Batch  240/355 - Train Accuracy: 0.6851, Validation Accuracy: 0.7178, Loss: 1.2728\n",
      "Epoch   4 Batch  250/355 - Train Accuracy: 0.7159, Validation Accuracy: 0.7234, Loss: 1.0326\n",
      "Epoch   4 Batch  260/355 - Train Accuracy: 0.7086, Validation Accuracy: 0.7233, Loss: 1.1235\n",
      "Epoch   4 Batch  270/355 - Train Accuracy: 0.6818, Validation Accuracy: 0.7236, Loss: 1.2848\n",
      "Epoch   4 Batch  280/355 - Train Accuracy: 0.7076, Validation Accuracy: 0.7258, Loss: 1.2016\n",
      "Epoch   4 Batch  290/355 - Train Accuracy: 0.3116, Validation Accuracy: 0.3104, Loss: 1.2298\n",
      "Epoch   4 Batch  300/355 - Train Accuracy: 0.7155, Validation Accuracy: 0.7245, Loss: 1.1037\n",
      "Epoch   4 Batch  310/355 - Train Accuracy: 0.6405, Validation Accuracy: 0.6552, Loss: 1.0679\n",
      "Epoch   4 Batch  320/355 - Train Accuracy: 0.6564, Validation Accuracy: 0.7147, Loss: 1.3127\n",
      "Epoch   4 Batch  330/355 - Train Accuracy: 0.4503, Validation Accuracy: 0.4004, Loss: 1.1230\n",
      "Epoch   4 Batch  340/355 - Train Accuracy: 0.5123, Validation Accuracy: 0.5386, Loss: 1.0207\n",
      "Epoch   4 Batch  350/355 - Train Accuracy: 0.4145, Validation Accuracy: 0.4202, Loss: 1.0819\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch   10/355 - Train Accuracy: 0.7072, Validation Accuracy: 0.7230, Loss: 1.0927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch   20/355 - Train Accuracy: 0.6873, Validation Accuracy: 0.7171, Loss: 1.2559\n",
      "Epoch   5 Batch   30/355 - Train Accuracy: 0.7147, Validation Accuracy: 0.7258, Loss: 1.1151\n",
      "Epoch   5 Batch   40/355 - Train Accuracy: 0.7046, Validation Accuracy: 0.7200, Loss: 1.1171\n",
      "Epoch   5 Batch   50/355 - Train Accuracy: 0.7072, Validation Accuracy: 0.7239, Loss: 1.2742\n",
      "Epoch   5 Batch   60/355 - Train Accuracy: 0.6705, Validation Accuracy: 0.7183, Loss: 1.2707\n",
      "Epoch   5 Batch   70/355 - Train Accuracy: 0.7030, Validation Accuracy: 0.7252, Loss: 1.2047\n",
      "Epoch   5 Batch   80/355 - Train Accuracy: 0.7116, Validation Accuracy: 0.7264, Loss: 1.1575\n",
      "Epoch   5 Batch   90/355 - Train Accuracy: 0.3856, Validation Accuracy: 0.4075, Loss: 1.1792\n",
      "Epoch   5 Batch  100/355 - Train Accuracy: 0.7132, Validation Accuracy: 0.7203, Loss: 1.0461\n",
      "Epoch   5 Batch  110/355 - Train Accuracy: 0.7503, Validation Accuracy: 0.7239, Loss: 0.8747\n",
      "Epoch   5 Batch  120/355 - Train Accuracy: 0.6986, Validation Accuracy: 0.7145, Loss: 1.0574\n",
      "Epoch   5 Batch  130/355 - Train Accuracy: 0.7981, Validation Accuracy: 0.7144, Loss: 0.7628\n",
      "Epoch   5 Batch  140/355 - Train Accuracy: 0.6004, Validation Accuracy: 0.6526, Loss: 1.5046\n",
      "Epoch   5 Batch  150/355 - Train Accuracy: 0.7237, Validation Accuracy: 0.7255, Loss: 1.0757\n",
      "Epoch   5 Batch  160/355 - Train Accuracy: 0.7209, Validation Accuracy: 0.7210, Loss: 1.0222\n",
      "Epoch   5 Batch  170/355 - Train Accuracy: 0.6911, Validation Accuracy: 0.7236, Loss: 1.2555\n",
      "Epoch   5 Batch  180/355 - Train Accuracy: 0.6791, Validation Accuracy: 0.7230, Loss: 1.2972\n",
      "Epoch   5 Batch  190/355 - Train Accuracy: 0.4004, Validation Accuracy: 0.3909, Loss: 1.1853\n",
      "Epoch   5 Batch  200/355 - Train Accuracy: 0.2517, Validation Accuracy: 0.2828, Loss: 1.2486\n",
      "Epoch   5 Batch  210/355 - Train Accuracy: 0.7131, Validation Accuracy: 0.7228, Loss: 1.1236\n",
      "Epoch   5 Batch  220/355 - Train Accuracy: 0.6484, Validation Accuracy: 0.7240, Loss: 1.4765\n",
      "Epoch   5 Batch  230/355 - Train Accuracy: 0.7417, Validation Accuracy: 0.7248, Loss: 0.9705\n",
      "Epoch   5 Batch  240/355 - Train Accuracy: 0.6863, Validation Accuracy: 0.7194, Loss: 1.2516\n",
      "Epoch   5 Batch  250/355 - Train Accuracy: 0.7147, Validation Accuracy: 0.7210, Loss: 1.0200\n",
      "Epoch   5 Batch  260/355 - Train Accuracy: 0.4952, Validation Accuracy: 0.5080, Loss: 1.1157\n",
      "Epoch   5 Batch  270/355 - Train Accuracy: 0.6789, Validation Accuracy: 0.7153, Loss: 1.2583\n",
      "Epoch   5 Batch  280/355 - Train Accuracy: 0.7042, Validation Accuracy: 0.7200, Loss: 1.1810\n",
      "Epoch   5 Batch  290/355 - Train Accuracy: 0.2976, Validation Accuracy: 0.2826, Loss: 1.2019\n",
      "Epoch   5 Batch  300/355 - Train Accuracy: 0.7144, Validation Accuracy: 0.7204, Loss: 1.0865\n",
      "Epoch   5 Batch  310/355 - Train Accuracy: 0.4837, Validation Accuracy: 0.4736, Loss: 1.0547\n",
      "Epoch   5 Batch  320/355 - Train Accuracy: 0.6608, Validation Accuracy: 0.7197, Loss: 1.2899\n",
      "Epoch   5 Batch  330/355 - Train Accuracy: 0.4761, Validation Accuracy: 0.4358, Loss: 1.1034\n",
      "Epoch   5 Batch  340/355 - Train Accuracy: 0.4277, Validation Accuracy: 0.4369, Loss: 1.0015\n",
      "Epoch   5 Batch  350/355 - Train Accuracy: 0.7137, Validation Accuracy: 0.7206, Loss: 1.0658\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch   10/355 - Train Accuracy: 0.6982, Validation Accuracy: 0.7123, Loss: 1.0788\n",
      "Epoch   6 Batch   20/355 - Train Accuracy: 0.4207, Validation Accuracy: 0.4088, Loss: 1.2440\n",
      "Epoch   6 Batch   30/355 - Train Accuracy: 0.7135, Validation Accuracy: 0.7246, Loss: 1.0972\n",
      "Epoch   6 Batch   40/355 - Train Accuracy: 0.7043, Validation Accuracy: 0.7194, Loss: 1.0929\n",
      "Epoch   6 Batch   50/355 - Train Accuracy: 0.7106, Validation Accuracy: 0.7237, Loss: 1.2422\n",
      "Epoch   6 Batch   60/355 - Train Accuracy: 0.6664, Validation Accuracy: 0.7188, Loss: 1.2526\n",
      "Epoch   6 Batch   70/355 - Train Accuracy: 0.6995, Validation Accuracy: 0.7183, Loss: 1.1866\n",
      "Epoch   6 Batch   80/355 - Train Accuracy: 0.7113, Validation Accuracy: 0.7200, Loss: 1.1394\n",
      "Epoch   6 Batch   90/355 - Train Accuracy: 0.3348, Validation Accuracy: 0.3586, Loss: 1.1657\n",
      "Epoch   6 Batch  100/355 - Train Accuracy: 0.7123, Validation Accuracy: 0.7175, Loss: 1.0317\n",
      "Epoch   6 Batch  110/355 - Train Accuracy: 0.7486, Validation Accuracy: 0.7201, Loss: 0.8601\n",
      "Epoch   6 Batch  120/355 - Train Accuracy: 0.7007, Validation Accuracy: 0.7180, Loss: 1.0429\n",
      "Epoch   6 Batch  130/355 - Train Accuracy: 0.7986, Validation Accuracy: 0.7160, Loss: 0.7523\n",
      "Epoch   6 Batch  140/355 - Train Accuracy: 0.6208, Validation Accuracy: 0.6928, Loss: 1.4814\n",
      "Epoch   6 Batch  150/355 - Train Accuracy: 0.7261, Validation Accuracy: 0.7215, Loss: 1.0577\n",
      "Epoch   6 Batch  160/355 - Train Accuracy: 0.7183, Validation Accuracy: 0.7204, Loss: 1.0102\n",
      "Epoch   6 Batch  170/355 - Train Accuracy: 0.6938, Validation Accuracy: 0.7201, Loss: 1.2325\n",
      "Epoch   6 Batch  180/355 - Train Accuracy: 0.6776, Validation Accuracy: 0.7181, Loss: 1.2816\n",
      "Epoch   6 Batch  190/355 - Train Accuracy: 0.4333, Validation Accuracy: 0.4136, Loss: 1.1671\n",
      "Epoch   6 Batch  200/355 - Train Accuracy: 0.2233, Validation Accuracy: 0.2550, Loss: 1.2285\n",
      "Epoch   6 Batch  210/355 - Train Accuracy: 0.7133, Validation Accuracy: 0.7227, Loss: 1.1087\n",
      "Epoch   6 Batch  220/355 - Train Accuracy: 0.6489, Validation Accuracy: 0.7210, Loss: 1.4519\n",
      "Epoch   6 Batch  230/355 - Train Accuracy: 0.7425, Validation Accuracy: 0.7225, Loss: 0.9558\n",
      "Epoch   6 Batch  240/355 - Train Accuracy: 0.6872, Validation Accuracy: 0.7194, Loss: 1.2291\n",
      "Epoch   6 Batch  250/355 - Train Accuracy: 0.7109, Validation Accuracy: 0.7194, Loss: 1.0016\n",
      "Epoch   6 Batch  260/355 - Train Accuracy: 0.6238, Validation Accuracy: 0.6456, Loss: 1.0992\n",
      "Epoch   6 Batch  270/355 - Train Accuracy: 0.6800, Validation Accuracy: 0.7168, Loss: 1.2397\n",
      "Epoch   6 Batch  280/355 - Train Accuracy: 0.7061, Validation Accuracy: 0.7206, Loss: 1.1612\n",
      "Epoch   6 Batch  290/355 - Train Accuracy: 0.3056, Validation Accuracy: 0.2856, Loss: 1.1843\n",
      "Epoch   6 Batch  300/355 - Train Accuracy: 0.7097, Validation Accuracy: 0.7160, Loss: 1.0694\n",
      "Epoch   6 Batch  310/355 - Train Accuracy: 0.4520, Validation Accuracy: 0.4138, Loss: 1.0440\n",
      "Epoch   6 Batch  320/355 - Train Accuracy: 0.6600, Validation Accuracy: 0.7200, Loss: 1.2661\n",
      "Epoch   6 Batch  330/355 - Train Accuracy: 0.4820, Validation Accuracy: 0.4380, Loss: 1.0904\n",
      "Epoch   6 Batch  340/355 - Train Accuracy: 0.6178, Validation Accuracy: 0.6158, Loss: 0.9857\n",
      "Epoch   6 Batch  350/355 - Train Accuracy: 0.7086, Validation Accuracy: 0.7180, Loss: 1.0494\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch   10/355 - Train Accuracy: 0.5237, Validation Accuracy: 0.5102, Loss: 1.0709\n",
      "Epoch   7 Batch   20/355 - Train Accuracy: 0.5267, Validation Accuracy: 0.5056, Loss: 1.2233\n",
      "Epoch   7 Batch   30/355 - Train Accuracy: 0.7135, Validation Accuracy: 0.7242, Loss: 1.0821\n",
      "Epoch   7 Batch   40/355 - Train Accuracy: 0.7037, Validation Accuracy: 0.7210, Loss: 1.0771\n",
      "Epoch   7 Batch   50/355 - Train Accuracy: 0.7114, Validation Accuracy: 0.7218, Loss: 1.2147\n",
      "Epoch   7 Batch   60/355 - Train Accuracy: 0.3603, Validation Accuracy: 0.3379, Loss: 1.2304\n",
      "Epoch   7 Batch   70/355 - Train Accuracy: 0.7033, Validation Accuracy: 0.7222, Loss: 1.1680\n",
      "Epoch   7 Batch   80/355 - Train Accuracy: 0.7120, Validation Accuracy: 0.7198, Loss: 1.1254\n",
      "Epoch   7 Batch   90/355 - Train Accuracy: 0.3939, Validation Accuracy: 0.4114, Loss: 1.1463\n",
      "Epoch   7 Batch  100/355 - Train Accuracy: 0.7138, Validation Accuracy: 0.7195, Loss: 1.0165\n",
      "Epoch   7 Batch  110/355 - Train Accuracy: 0.7442, Validation Accuracy: 0.7186, Loss: 0.8511\n",
      "Epoch   7 Batch  120/355 - Train Accuracy: 0.7018, Validation Accuracy: 0.7203, Loss: 1.0245\n",
      "Epoch   7 Batch  130/355 - Train Accuracy: 0.7991, Validation Accuracy: 0.7154, Loss: 0.7382\n",
      "Epoch   7 Batch  140/355 - Train Accuracy: 0.5137, Validation Accuracy: 0.5176, Loss: 1.4616\n",
      "Epoch   7 Batch  150/355 - Train Accuracy: 0.7269, Validation Accuracy: 0.7230, Loss: 1.0442\n",
      "Epoch   7 Batch  160/355 - Train Accuracy: 0.7180, Validation Accuracy: 0.7200, Loss: 0.9905\n",
      "Epoch   7 Batch  170/355 - Train Accuracy: 0.6932, Validation Accuracy: 0.7225, Loss: 1.2141\n",
      "Epoch   7 Batch  180/355 - Train Accuracy: 0.6696, Validation Accuracy: 0.7081, Loss: 1.2532\n",
      "Epoch   7 Batch  190/355 - Train Accuracy: 0.4859, Validation Accuracy: 0.4742, Loss: 1.1463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 Batch  200/355 - Train Accuracy: 0.3269, Validation Accuracy: 0.3367, Loss: 1.2124\n",
      "Epoch   7 Batch  210/355 - Train Accuracy: 0.7156, Validation Accuracy: 0.7228, Loss: 1.0897\n",
      "Epoch   7 Batch  220/355 - Train Accuracy: 0.6474, Validation Accuracy: 0.7215, Loss: 1.4233\n",
      "Epoch   7 Batch  230/355 - Train Accuracy: 0.7414, Validation Accuracy: 0.7230, Loss: 0.9414\n",
      "Epoch   7 Batch  240/355 - Train Accuracy: 0.6860, Validation Accuracy: 0.7172, Loss: 1.2081\n",
      "Epoch   7 Batch  250/355 - Train Accuracy: 0.7128, Validation Accuracy: 0.7227, Loss: 0.9867\n",
      "Epoch   7 Batch  260/355 - Train Accuracy: 0.6183, Validation Accuracy: 0.6259, Loss: 1.0789\n",
      "Epoch   7 Batch  270/355 - Train Accuracy: 0.6801, Validation Accuracy: 0.7234, Loss: 1.2192\n",
      "Epoch   7 Batch  280/355 - Train Accuracy: 0.5789, Validation Accuracy: 0.5702, Loss: 1.1395\n",
      "Epoch   7 Batch  290/355 - Train Accuracy: 0.3654, Validation Accuracy: 0.3083, Loss: 1.1620\n",
      "Epoch   7 Batch  300/355 - Train Accuracy: 0.7184, Validation Accuracy: 0.7219, Loss: 1.0498\n",
      "Epoch   7 Batch  310/355 - Train Accuracy: 0.3494, Validation Accuracy: 0.3368, Loss: 1.0329\n",
      "Epoch   7 Batch  320/355 - Train Accuracy: 0.6594, Validation Accuracy: 0.7207, Loss: 1.2470\n",
      "Epoch   7 Batch  330/355 - Train Accuracy: 0.4091, Validation Accuracy: 0.3591, Loss: 1.0730\n",
      "Epoch   7 Batch  340/355 - Train Accuracy: 0.5086, Validation Accuracy: 0.4570, Loss: 0.9776\n",
      "Epoch   7 Batch  350/355 - Train Accuracy: 0.5314, Validation Accuracy: 0.5035, Loss: 1.0370\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch   10/355 - Train Accuracy: 0.4830, Validation Accuracy: 0.4540, Loss: 1.0549\n",
      "Epoch   8 Batch   20/355 - Train Accuracy: 0.6880, Validation Accuracy: 0.7209, Loss: 1.2086\n",
      "Epoch   8 Batch   30/355 - Train Accuracy: 0.7117, Validation Accuracy: 0.7222, Loss: 1.0620\n",
      "Epoch   8 Batch   40/355 - Train Accuracy: 0.7078, Validation Accuracy: 0.7231, Loss: 1.0576\n",
      "Epoch   8 Batch   50/355 - Train Accuracy: 0.7141, Validation Accuracy: 0.7221, Loss: 1.1826\n",
      "Epoch   8 Batch   60/355 - Train Accuracy: 0.3344, Validation Accuracy: 0.3063, Loss: 1.2070\n",
      "Epoch   8 Batch   70/355 - Train Accuracy: 0.7009, Validation Accuracy: 0.7252, Loss: 1.1458\n",
      "Epoch   8 Batch   80/355 - Train Accuracy: 0.6966, Validation Accuracy: 0.6911, Loss: 1.1031\n",
      "Epoch   8 Batch   90/355 - Train Accuracy: 0.3283, Validation Accuracy: 0.3618, Loss: 1.1278\n",
      "Epoch   8 Batch  100/355 - Train Accuracy: 0.7120, Validation Accuracy: 0.7178, Loss: 1.0028\n",
      "Epoch   8 Batch  110/355 - Train Accuracy: 0.7280, Validation Accuracy: 0.7099, Loss: 0.8439\n",
      "Epoch   8 Batch  120/355 - Train Accuracy: 0.7025, Validation Accuracy: 0.7209, Loss: 1.0116\n",
      "Epoch   8 Batch  130/355 - Train Accuracy: 0.7987, Validation Accuracy: 0.7160, Loss: 0.7204\n",
      "Epoch   8 Batch  140/355 - Train Accuracy: 0.4247, Validation Accuracy: 0.3932, Loss: 1.4357\n",
      "Epoch   8 Batch  150/355 - Train Accuracy: 0.7273, Validation Accuracy: 0.7233, Loss: 1.0323\n",
      "Epoch   8 Batch  160/355 - Train Accuracy: 0.7180, Validation Accuracy: 0.7192, Loss: 0.9763\n",
      "Epoch   8 Batch  170/355 - Train Accuracy: 0.6328, Validation Accuracy: 0.6167, Loss: 1.1950\n",
      "Epoch   8 Batch  180/355 - Train Accuracy: 0.6322, Validation Accuracy: 0.6672, Loss: 1.2401\n",
      "Epoch   8 Batch  190/355 - Train Accuracy: 0.4892, Validation Accuracy: 0.4782, Loss: 1.1275\n",
      "Epoch   8 Batch  200/355 - Train Accuracy: 0.4244, Validation Accuracy: 0.4352, Loss: 1.1871\n",
      "Epoch   8 Batch  210/355 - Train Accuracy: 0.7142, Validation Accuracy: 0.7180, Loss: 1.0689\n",
      "Epoch   8 Batch  220/355 - Train Accuracy: 0.6453, Validation Accuracy: 0.7194, Loss: 1.3938\n",
      "Epoch   8 Batch  230/355 - Train Accuracy: 0.7419, Validation Accuracy: 0.7225, Loss: 0.9291\n",
      "Epoch   8 Batch  240/355 - Train Accuracy: 0.6866, Validation Accuracy: 0.7159, Loss: 1.1907\n",
      "Epoch   8 Batch  250/355 - Train Accuracy: 0.6480, Validation Accuracy: 0.6620, Loss: 0.9708\n",
      "Epoch   8 Batch  260/355 - Train Accuracy: 0.6950, Validation Accuracy: 0.7183, Loss: 1.0577\n",
      "Epoch   8 Batch  270/355 - Train Accuracy: 0.6824, Validation Accuracy: 0.7204, Loss: 1.1973\n",
      "Epoch   8 Batch  280/355 - Train Accuracy: 0.5434, Validation Accuracy: 0.5114, Loss: 1.1258\n",
      "Epoch   8 Batch  290/355 - Train Accuracy: 0.6692, Validation Accuracy: 0.6798, Loss: 1.1388\n",
      "Epoch   8 Batch  300/355 - Train Accuracy: 0.7170, Validation Accuracy: 0.7218, Loss: 1.0332\n",
      "Epoch   8 Batch  310/355 - Train Accuracy: 0.2459, Validation Accuracy: 0.2336, Loss: 1.0203\n",
      "Epoch   8 Batch  320/355 - Train Accuracy: 0.6605, Validation Accuracy: 0.7216, Loss: 1.2310\n",
      "Epoch   8 Batch  330/355 - Train Accuracy: 0.3425, Validation Accuracy: 0.2799, Loss: 1.0614\n",
      "Epoch   8 Batch  340/355 - Train Accuracy: 0.5608, Validation Accuracy: 0.5191, Loss: 0.9610\n",
      "Epoch   8 Batch  350/355 - Train Accuracy: 0.3220, Validation Accuracy: 0.2975, Loss: 1.0161\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch   10/355 - Train Accuracy: 0.3993, Validation Accuracy: 0.3643, Loss: 1.0382\n",
      "Epoch   9 Batch   20/355 - Train Accuracy: 0.6877, Validation Accuracy: 0.7225, Loss: 1.1942\n",
      "Epoch   9 Batch   30/355 - Train Accuracy: 0.7100, Validation Accuracy: 0.7212, Loss: 1.0466\n",
      "Epoch   9 Batch   40/355 - Train Accuracy: 0.7069, Validation Accuracy: 0.7198, Loss: 1.0445\n",
      "Epoch   9 Batch   50/355 - Train Accuracy: 0.7139, Validation Accuracy: 0.7204, Loss: 1.1535\n",
      "Epoch   9 Batch   60/355 - Train Accuracy: 0.3820, Validation Accuracy: 0.3535, Loss: 1.1917\n",
      "Epoch   9 Batch   70/355 - Train Accuracy: 0.7017, Validation Accuracy: 0.7258, Loss: 1.1272\n",
      "Epoch   9 Batch   80/355 - Train Accuracy: 0.7072, Validation Accuracy: 0.7178, Loss: 1.0864\n",
      "Epoch   9 Batch   90/355 - Train Accuracy: 0.3006, Validation Accuracy: 0.3265, Loss: 1.1077\n",
      "Epoch   9 Batch  100/355 - Train Accuracy: 0.7127, Validation Accuracy: 0.7201, Loss: 0.9848\n",
      "Epoch   9 Batch  110/355 - Train Accuracy: 0.7427, Validation Accuracy: 0.7154, Loss: 0.8426\n",
      "Epoch   9 Batch  120/355 - Train Accuracy: 0.7020, Validation Accuracy: 0.7188, Loss: 1.0013\n",
      "Epoch   9 Batch  130/355 - Train Accuracy: 0.7986, Validation Accuracy: 0.7153, Loss: 0.7094\n",
      "Epoch   9 Batch  140/355 - Train Accuracy: 0.3505, Validation Accuracy: 0.2892, Loss: 1.4125\n",
      "Epoch   9 Batch  150/355 - Train Accuracy: 0.7291, Validation Accuracy: 0.7228, Loss: 1.0089\n",
      "Epoch   9 Batch  160/355 - Train Accuracy: 0.6927, Validation Accuracy: 0.6786, Loss: 0.9636\n",
      "Epoch   9 Batch  170/355 - Train Accuracy: 0.5472, Validation Accuracy: 0.5089, Loss: 1.1829\n",
      "Epoch   9 Batch  180/355 - Train Accuracy: 0.6537, Validation Accuracy: 0.6791, Loss: 1.2146\n",
      "Epoch   9 Batch  190/355 - Train Accuracy: 0.6462, Validation Accuracy: 0.6477, Loss: 1.1152\n",
      "Epoch   9 Batch  200/355 - Train Accuracy: 0.4387, Validation Accuracy: 0.4435, Loss: 1.1729\n",
      "Epoch   9 Batch  210/355 - Train Accuracy: 0.5387, Validation Accuracy: 0.5207, Loss: 1.0549\n",
      "Epoch   9 Batch  220/355 - Train Accuracy: 0.6412, Validation Accuracy: 0.7148, Loss: 1.3636\n",
      "Epoch   9 Batch  230/355 - Train Accuracy: 0.7431, Validation Accuracy: 0.7210, Loss: 0.9213\n",
      "Epoch   9 Batch  240/355 - Train Accuracy: 0.6881, Validation Accuracy: 0.7169, Loss: 1.1692\n",
      "Epoch   9 Batch  250/355 - Train Accuracy: 0.4284, Validation Accuracy: 0.4041, Loss: 0.9627\n",
      "Epoch   9 Batch  260/355 - Train Accuracy: 0.7003, Validation Accuracy: 0.7225, Loss: 1.0487\n",
      "Epoch   9 Batch  270/355 - Train Accuracy: 0.6765, Validation Accuracy: 0.7103, Loss: 1.1786\n",
      "Epoch   9 Batch  280/355 - Train Accuracy: 0.3609, Validation Accuracy: 0.3238, Loss: 1.1074\n",
      "Epoch   9 Batch  290/355 - Train Accuracy: 0.6976, Validation Accuracy: 0.7192, Loss: 1.1105\n",
      "Epoch   9 Batch  300/355 - Train Accuracy: 0.7181, Validation Accuracy: 0.7239, Loss: 1.0210\n",
      "Epoch   9 Batch  310/355 - Train Accuracy: 0.2679, Validation Accuracy: 0.2719, Loss: 1.0058\n",
      "Epoch   9 Batch  320/355 - Train Accuracy: 0.6636, Validation Accuracy: 0.7168, Loss: 1.2243\n",
      "Epoch   9 Batch  330/355 - Train Accuracy: 0.6068, Validation Accuracy: 0.5428, Loss: 1.0477\n",
      "Epoch   9 Batch  340/355 - Train Accuracy: 0.5303, Validation Accuracy: 0.4871, Loss: 0.9359\n",
      "Epoch   9 Batch  350/355 - Train Accuracy: 0.3647, Validation Accuracy: 0.3317, Loss: 0.9987\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Split data to training and validation sets\n",
    "\n",
    "# use only one batch as test batch \n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "\n",
    "# rest datat for train\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "# padding each batch for valid data\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "# create a TensorFlow session to run parts of the graph   \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # for each padded batch in training data\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "        # Save Model each epochs\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path)\n",
    "        print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    with open('models/params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "        \n",
    "    \n",
    "# Save parameters for checkpoint\n",
    "save_params(save_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
