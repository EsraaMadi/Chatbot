{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Chatbot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design and build a simple chatbot using data from the Cornell Movie Dialogues corpus, using Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model structure\n",
    "In short, the input sequence (the question asked to the chatbot) is passed into the encder LSTM, which outputs the final states of the encoder LSTM. These final states are passed into the decoder LSTM, along with the output sequence (the reply for the question, in the training data). The output of this decoder LSTM is the same as the actual reply, but shifted one time step to the left. That is, if the reply (aka, the input to the decoder lstm) is 'I am fine', the output for first time step with input 'I' will be 'am', the input for the second time step will be 'am', with output 'fine', and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](https://cdn-images-1.medium.com/max/1600/1*Ismhi-muID5ooWf3ZIQFFg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Number of Epochs\n",
    "NUM_EPOCHS = 25 #100\n",
    "\n",
    "# GLOVE Embedding Size\n",
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "\n",
    "# RNN Size\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# vocabulary size\n",
    "MAX_VOCAB_SIZE = 14000 #1000\n",
    "\n",
    "# Model file path\n",
    "WEIGHT_FILE_PATH = 'models/keras-glove-weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them\n",
    "    \"\"\"\n",
    "    with open('models/preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call loading function\n",
    "((context),\n",
    "(input_texts_word2em),\n",
    "(target_texts),\n",
    "(word2em),\n",
    "(target_word2idx, target_idx2word)) = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens = context['num_decoder_tokens']\n",
    "encoder_max_seq_length = context['encoder_max_seq_length']\n",
    "decoder_max_seq_length = context['decoder_max_seq_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Input batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(input_word2em_data, output_text_data):\n",
    "    '''\n",
    "    '''\n",
    "    num_batches = len(input_word2em_data) // BATCH_SIZE\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE # until end of next batch\n",
    "            encoder_input_data_batch = pad_sequences(input_word2em_data[start:end], encoder_max_seq_length)\n",
    "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, GLOVE_EMBEDDING_SIZE))\n",
    "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = target_word2idx['unknown']  # default unknown\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    if w in word2em:\n",
    "                        decoder_input_data_batch[lineIdx, idx, :] = word2em[w]\n",
    "                    if idx > 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Neural Network Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# decoder\n",
    "decoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                 initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# pass inputs to model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "json = model.to_json()\n",
    "pickle.dump((json), open('models/keras-glove-architecture.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(input_texts_word2em, target_texts, test_size=0.2)\n",
    "\n",
    "print(len(Xtrain))\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86508\n",
      "21628\n",
      "Epoch 1/25\n",
      "1351/1351 [==============================] - 714s 528ms/step - loss: 1.5275 - val_loss: 1.4153\n",
      "Epoch 2/25\n",
      "1351/1351 [==============================] - 693s 513ms/step - loss: 1.3903 - val_loss: 1.3661\n",
      "Epoch 3/25\n",
      "1351/1351 [==============================] - 704s 521ms/step - loss: 1.3503 - val_loss: 1.3465\n",
      "Epoch 4/25\n",
      "1351/1351 [==============================] - 717s 530ms/step - loss: 1.3260 - val_loss: 1.3397\n",
      "Epoch 5/25\n",
      " 740/1351 [===============>..............] - ETA: 4:45 - loss: 1.3051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351/1351 [==============================] - 715s 529ms/step - loss: 1.2779 - val_loss: 1.3385\n",
      "Epoch 9/25\n",
      "1351/1351 [==============================] - 716s 530ms/step - loss: 1.2724 - val_loss: 1.3416\n",
      "Epoch 10/25\n",
      "1100/1351 [=======================>......] - ETA: 1:56 - loss: 1.2670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351/1351 [==============================] - 716s 530ms/step - loss: 1.2569 - val_loss: 1.3542\n",
      "Epoch 14/25\n",
      "1351/1351 [==============================] - 716s 530ms/step - loss: 1.2567 - val_loss: 1.3633\n",
      "Epoch 15/25\n",
      "1351/1351 [==============================] - 714s 529ms/step - loss: 1.2532 - val_loss: 1.3649\n",
      "Epoch 16/25\n",
      "1351/1351 [==============================] - 712s 527ms/step - loss: 1.2481 - val_loss: 1.3695\n",
      "Epoch 17/25\n",
      "1351/1351 [==============================] - 717s 531ms/step - loss: 1.2446 - val_loss: 1.3736\n",
      "Epoch 18/25\n",
      "1351/1351 [==============================] - 717s 531ms/step - loss: 1.2431 - val_loss: 1.3786\n",
      "Epoch 19/25\n",
      "1351/1351 [==============================] - 712s 527ms/step - loss: 1.2423 - val_loss: 1.3832\n",
      "Epoch 20/25\n",
      "1351/1351 [==============================] - 716s 530ms/step - loss: 1.2401 - val_loss: 1.3865\n",
      "Epoch 21/25\n",
      "1351/1351 [==============================] - 712s 527ms/step - loss: 1.2397 - val_loss: 1.3911\n",
      "Epoch 22/25\n",
      "1351/1351 [==============================] - 711s 526ms/step - loss: 1.2358 - val_loss: 1.3929\n",
      "Epoch 23/25\n",
      "1351/1351 [==============================] - 707s 524ms/step - loss: 1.2337 - val_loss: 1.3940\n",
      "Epoch 24/25\n",
      "1351/1351 [==============================] - 705s 522ms/step - loss: 1.2307 - val_loss: 1.4009\n",
      "Epoch 25/25\n",
      "1351/1351 [==============================] - 703s 521ms/step - loss: 1.2297 - val_loss: 1.4031\n"
     ]
    }
   ],
   "source": [
    "# get train and test batches\n",
    "train_gen = generate_batch(Xtrain, Ytrain)\n",
    "test_gen = generate_batch(Xtest, Ytest)\n",
    "\n",
    "# batches number \n",
    "train_num_batches = len(Xtrain) // BATCH_SIZE\n",
    "test_num_batches = len(Xtest) // BATCH_SIZE\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
    "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches, callbacks=[checkpoint])\n",
    "\n",
    "model.save_weights(WEIGHT_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few challenges in using this model. The most disturbing one is that the model cannot handle variable length sequences.  The next one is the vocabulary size. The decoder has to run softmax over a large vocabulary for each word in the output. That is going to slow down the training process, even if your hardware is capable of handling it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
