{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal (Chatbot)\n",
    "Esraa Madi  \n",
    "Dec 7, 2018\n",
    "\n",
    "## Proposal\n",
    "\n",
    "\n",
    "### Domain Background\n",
    "> Since I am going to build a chatbot, Optical Character Recognition (OCR) is the main part of my project. OCR detects text in an image and extracts the recognized words into a machine-readable character stream. Analyse images to detect embedded text, generate character streams and enable searching. Take photos of text instead of copying to save time and effort <br>\n",
    "> There are many OCR from different-2 MNCs and start-ups like Google, Microsoft, Tesseract and many more.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "> People by nature like to share happiness and sadness moments with each other. Nowadays with the internet and social media, they tend to share every single moment with their followers including foods, places and feeling, literally everything, so they sacrificed with their privacy just to get feedback and compliments from others.  \n",
    "\n",
    ">As an unsocial person and don't like to post my life on the social media, I want a bot is able to handel the normal sentances and response with a reply it can be considr it as human answer not machine answer.\n",
    "\n",
    "\n",
    "### Datasets and Inputs\n",
    "\n",
    ">I need a large text dataset to create my bot own vocabulary that should be used to understand and replay to the user.\n",
    ">For getting the bot training data, there are quite a few resources I could look into. In my project I will use the most popular dataset:\n",
    "\n",
    "> 1. Cornell movie dialogue corpus\n",
    "> 2. Reddit comments: I want something that is more raw. so i will try to use Reddit.\n",
    "\n",
    "> **How to Collect data:**\n",
    "> 1. Cornell movie dialogue corpus: is available [here](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). it contains 2 files, the first file contains all sentences and corresponding ids. The second file contains the sequence of all conversations as ids.\n",
    "> 2. Reddit comments : At first,I thought I would use the Python Reddit API Wrapper, but the limits imposed by Reddit on crawling are not the most friendly. To collect bulk amounts of data. Instead, I found a data dump of Reddit Comments separated by month as [JSON files](https://files.pushshift.io/reddit/comments/), after some investigation i found some comments with 1 reply per comment and the other might have many replies, so i decided to go with the top-voted one.\n",
    "\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    ">To build my chatbot, i will start by build a large vocabulary which is needed to understand sentences. then i will apply NLP techniques to preprocessing the text and train my model on these preprocessed sentence . I am going to integrate it into my slack so the chatbot could talk to people in real-time.  The bot is guaranteed to **generate** (more or less fluent) sentence, not matching input sentence to get the closest trained answer.\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    "> after long search i found this slack bot https://github.com/juliakreutzer/neural-slack-bot that i can use it as Benchmark Model\n",
    "\n",
    "### Evaluation Metrics\n",
    "> 1. Comprehension capabilities: <br>\n",
    "Good comprehension capabilities of a chatbot should ensure a good texting and error free experience for the user. When a user types a spelling mistake or makes an error in a sentence, the chatbot should enable the â€˜auto-correctâ€™ feature.\n",
    ">2. User engagement: <br>\n",
    "Good chatbots should be capable of initiating conversation with the users and interact with them to share information. Measuring engagement should give you a better idea of how well your chatbot is performing and delivering meaningful messages.\n",
    ">3. Speed: <br>\n",
    "Measuring the response rate of the chatbot plays an important role when it comes to speed. Quality chatbots should be capable of delivering responses immediately for effective interactions.\n",
    "\n",
    "### Project Design\n",
    "\n",
    ">**EDA (Prepare the Data):**<br>\n",
    ">In order to prepare the data for training, we need to use a special script to convert the data to the format which is required in order to train my model.\n",
    ">1. Creating Dataset:\n",
    "    - I need to create 2 files, first file with all questions sentances and the second one with suitable response sentences from both movie data and Reddit data\n",
    "    - By the end of this step , I would have 4 files that will be used of the training and testing:\n",
    "        - train.from, test.from (chatbot input)\n",
    "        - train.to, test.to (chatbot output)<br>\n",
    ">2. Cleaning Text: (apply on all sentances on both files)\n",
    "    - Remove punctuations.\n",
    "    - Remove non English words.\n",
    "    - Remove urls.<br>\n",
    ">3. Creating Vocabulary: \n",
    "    - The chatbot Vocabulary contains all unique words in my dataset and giving each word an id. <br>\n",
    ">4. Converting Observations:\n",
    "    - Since i have 2 files , one for questions sentences and another for response sentences, I need to convert each word in each sentence to its corresponding id using the above vocabulary.<br>\n",
    "    \n",
    ">**Modeling:**<br>\n",
    ">In this project, I am going to build a Neural Conversational Model by training a recurrent neural network based chatbot from scratch.I will use Sequence To Sequence model. It consists of two RNNs: An Encoder and a Decoder. The encoder takes a sequence(sentence) as input and processes one symbol(word) at each timestep. Its objective is to convert a sequence of symbols into a fixed size feature vector that encodes only the important information in the sequence while losing the unnecessary information. The decoder takes this fixed size vector and produces a response (word)<br>\n",
    "\n",
    "\n",
    ">**Main chatbot functions:**<br>\n",
    ">1. Design and build a chatbot using collected data from scratch, that can generate a good text as a result.\n",
    ">2. Integrated with Slack as an interface for my chatbot.\n",
    ">3. Chatbot input could be a text or an image (note) , so I am going to use [Google API](https://cloud.google.com/vision/) to extract text from images.\n",
    "> 4. Chatbot output could be a text or a gif, so I am going to use [GIPHY](https://giphy.com/search/funny-photoshoot) to generate funny gif by passing the main words in the generated text.\n",
    "> 5. If the score of chatbot output for a given sentence is under the baseline score, The chatbot will respond with Â¯\\_(ãƒ„)_/Â¯ ðŸ¤· as a default response<br>\n",
    "> 6. **If I have time:** Chatbot could recognize people name in the conversation using [Google API](https://cloud.google.com/natural-language/) then its response depends on the previous conversations about them (maybe I will save the chat history which is related to people and use it to get the response<br>\n",
    "\n",
    ">**Model Structure:**<br>\n",
    ">![alt text](images/model.png \"Chatbot Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
