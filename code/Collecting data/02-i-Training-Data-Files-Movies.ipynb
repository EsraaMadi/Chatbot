{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Movies data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the data for training, we need to convert the data to the format, that is required in order to train my RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of the execution, I would have 4 files that will be used of the training and testing:\n",
    "- Trining Files:\n",
    "    - train.from (chatbot input)\n",
    "    - train.to (chatbot output)\n",
    "- Test Files:\n",
    "    - test.from (chatbot input)\n",
    "    - test.to (chatbot output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines():\n",
    "    '''\n",
    "    1. Read from 'movie-lines.txt'\n",
    "    2. Create a dictionary with ( key = line_id, value = text )\n",
    "    '''\n",
    "    \n",
    "    lines=open('../data/raw_data/movies/movie_lines.txt',\n",
    "               encoding='utf-8',\n",
    "               errors='ignore').read().split('\\n')[:-1]\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orderd_conversations():\n",
    "    '''\n",
    "    1. Read from 'movie_conversations.txt'\n",
    "    2. Create a list of [list of line_id's]\n",
    "    '''\n",
    "    conv_lines = open('../data/raw_data/movies/movie_conversations.txt',\n",
    "                      encoding='utf-8',\n",
    "                      errors='ignore').read().split('\\n')[:-1]\n",
    "    convs = [ ]\n",
    "    for line in conv_lines:\n",
    "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(convs, id2line):\n",
    "    '''\n",
    "    Get lists of all conversations as Questions and Answers\n",
    "    1. [questions]\n",
    "    2. [answers]\n",
    "    '''\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for conv in convs:\n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                questions.append(id2line[conv[i]])\n",
    "            else:\n",
    "                answers.append(id2line[conv[i]])\n",
    "                \n",
    "    df = pd.DataFrame({'parent':questions,\n",
    "                       'comment':answers})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dict = get_lines()\n",
    "convs_lst = get_orderd_conversations()\n",
    "data = create_dataset(convs_lst, line_dict)\n",
    "\n",
    "test_size = 30000\n",
    "\n",
    "test_files = data[:test_size]\n",
    "train_files = data[test_size:]\n",
    "\n",
    "# test files\n",
    "# create a file for all parent text only\n",
    "with open('../data/datasets/movies/test.from','a', encoding='utf8') as f:\n",
    "    for content in test_files['parent'].values:\n",
    "        f.write(str(content)+'\\n')\n",
    "\n",
    "# create a file for all comment text only\n",
    "with open('../data/datasets/movies/test.to','a', encoding='utf8') as f:\n",
    "    for content in test_files['comment'].values:\n",
    "        f.write(str(content)+'\\n')\n",
    "\n",
    "\n",
    "# train files\n",
    "# create a file for all parent text only\n",
    "with open('../data/datasets/movies/train.from','a', encoding='utf8') as f:\n",
    "    for content in train_files['parent'].values:\n",
    "        f.write(str(content)+'\\n')\n",
    "\n",
    "# create a file for all comment text only\n",
    "with open('../data/datasets/movies/train.to','a', encoding='utf8') as f:\n",
    "    for content in train_files['comment'].values:\n",
    "        f.write(str(content)+'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
