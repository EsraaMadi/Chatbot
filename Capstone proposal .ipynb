{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal (Chatbot)\n",
    "Esraa Madi  \n",
    "Dec 1, 2018\n",
    "\n",
    "## Proposal\n",
    "\n",
    "\n",
    "### Domain Background\n",
    "\n",
    "> People by nature like to share happiness and sadness moments with each other. Nowadays with the internet and social media, they tend to share every single moment with their followers including foods, places and feeling, literally everything, so they sacrificed with their privacy just to get feedback and compliments from others.  \n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    ">As an unsocial person and don't like to post my life on the social media, I want a bot to share my daily occasions with it and be as a friend for me <br>\n",
    ">Everyday I write a sticky note for special occasions in my day, so i wanna to share these notes with someone to just get fun but in the same time i don't anyone (human) to see my notes because most of time they have sensitive info about me and the other people around me.\n",
    "\n",
    "### Datasets and Inputs\n",
    "\n",
    ">I need a large text dataset to create my bot own vocabulary that should be used to understand and replay to the user.\n",
    ">For getting the bot training data, there are quite a few resources I could look into. In my project I will use the most popular dataset:\n",
    "\n",
    "> 1. Cornell movie dialogue corpus\n",
    "> 2. Reddit comments: I want something that is more raw. so i will try to use Reddit.\n",
    "\n",
    "> **How to Collect data:**\n",
    "> 1. Cornell movie dialogue corpus: is available [here](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). it contains 2 files, the first file contains all sentences and corresponding ids. The second file contains the sequence of all conversations as ids.\n",
    "> 2. Reddit comments : At first,I thought I would use the Python Reddit API Wrapper, but the limits imposed by Reddit on crawling are not the most friendly. To collect bulk amounts of data. Instead, I found a data dump of Reddit Comments separated by month as [JSON files](https://files.pushshift.io/reddit/comments/), after some investigation i found some comments with 1 reply per comment and the other might have many replies, so i decided to go with the top-voted one.\n",
    "\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    ">In this project, I am going to chatbot by training  it to a large vocabulary. The chatbot could talk to people in real-time. I am going to integrate it into my slack.  The bot is guaranteed to **generate** (more or less fluent) sentence, not matching input sentence to get the closest trained answer\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    ">There is a new wave of startups trying to change how consumers interact with services by building consumer apps and bot platforms.<br>\n",
    ">Indeed, today chatbots are used to solve a number of business tasks across many industries like E-Commerce, Insurance, Banking, Healthcare, Finance, Legal, Telecom, Logistics, Retail, Auto, Leisure, Travel, Sports, Entertainment, Media and many others.<br>\n",
    ">The best-known platforms for building chatbots such as:\n",
    ">1. IBM Watson:<br>\n",
    " It is built on a neural network (one billion Wikipedia words), understands intents, interprets entities and dialogs, supports English and Japanese languages.\n",
    ">2. Microsoft Bot Framework:<br>\n",
    " It is open source and available to all on Github, and it supports automatic translation to more than 30 languages. Microsoft Bot Framework understands usersâ€™ intents. It is possible to incorporate LUIS for natural language understanding, Cortana for voice, and the Bing APIs for search.\n",
    " \n",
    "### Evaluation Metrics\n",
    "> 1. Comprehension capabilities: <br>\n",
    "Good comprehension capabilities of a chatbot should ensure a good texting and error free experience for the user. When a user types a spelling mistake or makes an error in a sentence, the chatbot should enable the â€˜auto-correctâ€™ feature.\n",
    ">2. User engagement: <br>\n",
    "Good chatbots should be capable of initiating conversation with the users and interact with them to share information. Measuring engagement should give you a better idea of how well your chatbot is performing and delivering meaningful messages.\n",
    ">3. Speed: <br>\n",
    "Measuring the response rate of the chatbot plays an important role when it comes to speed. Quality chatbots should be capable of delivering responses immediately for effective interactions.\n",
    "\n",
    "### Project Design\n",
    "\n",
    ">**EDA (Prepare the Data):**<br>\n",
    ">In order to prepare the data for training, we need to use a special script to convert the data to the format which is required in order to train my model.\n",
    ">1. Creating Dataset:\n",
    "    - I need to create 2 files, first file with all questions sentances and the second one with suitable response sentences from both movie data and Reddit data\n",
    "    - By the end of this step , I would have 4 files that will be used of the training and testing:\n",
    "        - train.from, test.from (chatbot input)\n",
    "        - train.to, test.to (chatbot output)<br>\n",
    ">2. Cleaning Text: (apply on all sentances on both files)\n",
    "    - Remove punctuations.\n",
    "    - Remove non English words.\n",
    "    - Remove urls.<br>\n",
    ">3. Creating Vocabulary: \n",
    "    - The chatbot Vocabulary contains all unique words in my dataset and giving each word an id. <br>\n",
    ">4. Converting Observations:\n",
    "    - Since i have 2 files , one for questions sentences and another for response sentences, I need to convert each word in each sentence to its corresponding id using the above vocabulary.<br>\n",
    "    \n",
    ">**Modeling:**<br>\n",
    ">In this project, I am going to build a Neural Conversational Model by training a recurrent neural network based chatbot from scratch.I will use Sequence To Sequence model. It consists of two RNNs: An Encoder and a Decoder. The encoder takes a sequence(sentence) as input and processes one symbol(word) at each timestep. Its objective is to convert a sequence of symbols into a fixed size feature vector that encodes only the important information in the sequence while losing the unnecessary information. The decoder takes this fixed size vector and produces a response (word)<br>\n",
    "\n",
    "\n",
    ">**Main chatbot functions:**<br>\n",
    ">1. Design and build a chatbot using collected data from scratch, that can generate a good text as a result.\n",
    ">2. Integrated with Slack as an interface for my chatbot.\n",
    ">3. Chatbot input could be a text or an image (note) , so I am going to use [Google API](https://cloud.google.com/vision/) to extract text from images.\n",
    "> 4. Chatbot output could be a text or a gif, so I am going to use [GIPHY](https://giphy.com/search/funny-photoshoot) to generate funny gif by passing the main words in the generated text.\n",
    "> 5. If the score of chatbot output for a given sentence is under the baseline score, The chatbot will respond with Â¯\\_(ãƒ„)_/Â¯ ðŸ¤· as a default response<br>\n",
    "> 6. **If I have time:** Chatbot could recognize people name in the conversation using [Google API](https://cloud.google.com/natural-language/) then its response depends on the previous conversations about them (maybe I will save the chat history which is related to people and use it to get the response<br>\n",
    "\n",
    ">**Model Structure:**<br>\n",
    ">![alt text](images/model.png \"Chatbot Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
