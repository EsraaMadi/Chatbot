{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with text data is to pre-process it. I cannot go straight from raw text to fitting a machine learning model. I must clean the text first, which means splitting it into words and handling punctuation and case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaing the text data, I used **Word Embedding** which is a technique for representation of words in a low dimensional vector space. Each word represents by a fixed length vector. Semantic relations between words are captured by this technique.\n",
    "there are many ways to implement the word space, one of them is using pre-build Word Embedding such as [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import zipfile\n",
    "import pickle\n",
    "import nltk\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE Embedding Size\n",
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "\n",
    "# Max number of words in each sentance (source)\n",
    "MAX_INPUT_SEQ_LENGTH =  40\n",
    "\n",
    "# Max number of words in each sentance (target)\n",
    "MAX_TARGET_SEQ_LENGTH = 40\n",
    "\n",
    "# vocabulary size\n",
    "MAX_VOCAB_SIZE = 14000 #1000\n",
    "\n",
    "# Dataset files path\n",
    "SOURCE_DATA_PATH = 'data/datasets/train.from'\n",
    "TARGET_DATA_PATH = 'data/datasets/train.to'\n",
    "\n",
    "# Glove files path\n",
    "GLOVE_MODEL = \"data/glove-data/glove.6B.\" + str(GLOVE_EMBEDDING_SIZE) + \"d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load Glove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporthook(block_num, block_size, total_size):\n",
    "    read_so_far = block_num * block_size\n",
    "    if total_size > 0:\n",
    "        percent = read_so_far * 1e2 / total_size\n",
    "        s = \"\\r%5.1f%% %*d / %d\" % (\n",
    "            percent, len(str(total_size)), read_so_far, total_size)\n",
    "        sys.stderr.write(s)\n",
    "        if read_so_far >= total_size:  # near the end\n",
    "            sys.stderr.write(\"\\n\")\n",
    "    else:  # total size is unknown\n",
    "        sys.stderr.write(\"read %d\\n\" % (read_so_far,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove():\n",
    "    '''\n",
    "    Function to download GloVe files if not exist\n",
    "    '''\n",
    "    if not os.path.exists(GLOVE_MODEL):\n",
    "\n",
    "        glove_zip = 'data/glove-data/glove.6B.zip'\n",
    "\n",
    "        if not os.path.exists('data/glove-data'):\n",
    "            os.makedirs('data/glove-data')\n",
    "\n",
    "        if not os.path.exists(glove_zip):\n",
    "            print('glove file does not exist, downloading from internet')\n",
    "            urllib.request.urlretrieve(url='http://nlp.stanford.edu/data/glove.6B.zip', filename=glove_zip,\n",
    "                                       reporthook=reporthook)\n",
    "\n",
    "        print('unzipping glove file')\n",
    "        zip_ref = zipfile.ZipFile(glove_zip, 'r')\n",
    "        zip_ref.extractall('data/glove-data')\n",
    "        zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    '''\n",
    "    Function to read gloVe files\n",
    "        * return: dict of all words and their embedding vectors\n",
    "    '''\n",
    "    # download gloVe files\n",
    "    download_glove()\n",
    "    \n",
    "    # dict with key= word , value = embedding vector 100\n",
    "    _word2em = {}\n",
    "    file = open(GLOVE_MODEL, mode='rt', encoding='utf8')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        _word2em[word] = embeds\n",
    "    file.close()\n",
    "    return _word2em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove file does not exist, downloading from internet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 862183424 / 862182613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzipping glove file\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "word2em = load_glove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    ''' Function to read training and testing files\n",
    "            *args:\n",
    "                path: file path as string \n",
    "            *return:\n",
    "                data: raw string text\n",
    "    '''\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original training data (comment, replay) Movies\n",
    "mov_source_text = load_data(SOURCE_DATA_PATH).lower()\n",
    "mov_target_text = load_data(TARGET_DATA_PATH).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data and Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_accurance(text, vocab_counter, is_padding):\n",
    "    '''\n",
    "    Function to create a counter for each word occurrence and list for all observation sentance\n",
    "        *args:\n",
    "            text: string\n",
    "            vocab_counter: Vocabulary counter\n",
    "            is_padding: boolean, do padding or not \n",
    "        *return:\n",
    "            text_lst: list of all observation sentences\n",
    "            vocab_counter: Vocabulary counter\n",
    "        \n",
    "    '''\n",
    "    text_lst = [] \n",
    "    for sen in text.split(\"\\n\"):\n",
    "        # split words\n",
    "        sen = [w for w in nltk.word_tokenize(sen)]\n",
    "        # check length of sentence\n",
    "        if len(sen) > MAX_TARGET_SEQ_LENGTH:\n",
    "            sen = sen[0:MAX_TARGET_SEQ_LENGTH]\n",
    "        # fill Vocabulary counter\n",
    "        for w in sen:\n",
    "            vocab_counter[w] += 1\n",
    "        # check padding request \n",
    "        if is_padding:\n",
    "            sen.insert(0, 'start')\n",
    "            sen.append('end')  \n",
    "        text_lst.append(sen) \n",
    "    return text_lst, vocab_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocab counter for each word in training data (comment, replay) \n",
    "vocab_counter = Counter()\n",
    "\n",
    "# preprocess comments\n",
    "input_texts, vocab_counter = vocab_accurance(mov_source_text, vocab_counter, is_padding=False)\n",
    "\n",
    "# preprocess replay\n",
    "target_texts, vocab_counter = vocab_accurance(mov_target_text, vocab_counter, is_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ['colonel', 'durnford', '...', 'william', 'vereker', '.', 'i', 'hear', 'you', \"'ve\", 'been', 'seeking', 'officers', '?']\n",
      "Target: ['start', 'good', 'ones', ',', 'yes', ',', 'mr', 'vereker', '.', 'gentlemen', 'who', 'can', 'ride', 'and', 'shoot', 'end'] \n",
      "\n",
      "Source: ['your', 'orders', ',', 'mr', 'vereker', '?']\n",
      "Target: ['start', 'i', \"'m\", 'to', 'take', 'the', 'sikali', 'with', 'the', 'main', 'column', 'to', 'the', 'river', 'end'] \n",
      "\n",
      "Source: ['lord', 'chelmsford', 'seems', 'to', 'want', 'me', 'to', 'stay', 'back', 'with', 'my', 'basutos', '.']\n",
      "Target: ['start', 'i', 'think', 'chelmsford', 'wants', 'a', 'good', 'man', 'on', 'the', 'border', 'why', 'he', 'fears', 'a', 'flanking', 'attack', 'and', 'requires', 'a', 'steady', 'commander', 'in', 'reserve', '.', 'end'] \n",
      "\n",
      "Source: ['well', 'i', 'assure', 'you', ',', 'sir', ',', 'i', 'have', 'no', 'desire', 'to', 'create', 'difficulties', '.', '45']\n",
      "Target: ['start', 'and', 'i', 'assure', 'you', ',', 'you', 'do', 'not', 'in', 'fact', 'i', \"'d\", 'be', 'obliged', 'for', 'your', 'best', 'advice', '.', 'what', 'have', 'your', 'scouts', 'seen', '?', 'end'] \n",
      "\n",
      "Source: []\n",
      "Target: ['start', 'end'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check data pairs after cleaning and padding start and end tags\n",
    "for idx, (input_words, target_words) in enumerate(zip(input_texts[-5:], target_texts[-5:])):\n",
    "    print('Source:', input_words)\n",
    "    print('Target:', target_words, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocab with most accurance words (more than 3 times)\n",
    "target_word2idx = dict()\n",
    "for idx, word in enumerate(vocab_counter.most_common(len(vocab_counter))):\n",
    "    # if wird accrue more than 3 times\n",
    "    if word[1] > 3:\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    \n",
    "\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should have many unknown words if MAX_VOCAB_SIZE < len(target_counter)\n",
    "if 'unknown' not in target_word2idx:\n",
    "    target_word2idx['unknown'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15856\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:',len(target_word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding comments words\n",
    "\n",
    "input_texts_word2em = []\n",
    "\n",
    "# longest sentance in the dataset\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "#embedding input of encoder\n",
    "for input_sentance, target_sentance in zip(input_texts, target_texts):\n",
    "    # each sentance\n",
    "    encoder_input_wids = []\n",
    "    for word in input_sentance:\n",
    "        # each word\n",
    "        emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
    "        if word in word2em:\n",
    "            emb = word2em[word]\n",
    "        encoder_input_wids.append(emb)\n",
    "\n",
    "    input_texts_word2em.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_sentance), decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens = len(target_idx2word)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_max_seq_length': 42, 'encoder_max_seq_length': 40, 'num_decoder_tokens': 15857}\n"
     ]
    }
   ],
   "source": [
    "context = dict()\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dicts\n",
    "pickle.dump(((context),\n",
    "             (input_texts_word2em),\n",
    "             (target_texts),\n",
    "             (word2em),\n",
    "             (target_word2idx, target_idx2word)), open('models/preprocess.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
