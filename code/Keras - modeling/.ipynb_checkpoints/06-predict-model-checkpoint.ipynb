{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Text using the Chatbot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](https://cdn-images-1.medium.com/max/1200/1*CUFxTTJ4M54YLRhwrlTjpw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import pickle\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Size\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# list of characters\n",
    "WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'\n",
    "\n",
    "# GLOVE Embedding Size\n",
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "\n",
    "# GLOVE file path\n",
    "GLOVE_MODEL = \"data/glove-data/glove.6B.\" + str(GLOVE_EMBEDDING_SIZE) + \"d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load Glove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove():\n",
    "    '''\n",
    "    Function to download GloVe files if not exist\n",
    "    '''\n",
    "    if not os.path.exists(GLOVE_MODEL):\n",
    "\n",
    "        glove_zip = 'data/glove-data/glove.6B.zip'\n",
    "\n",
    "        if not os.path.exists('data/glove-data'):\n",
    "            os.makedirs('data/glove-data')\n",
    "\n",
    "        if not os.path.exists(glove_zip):\n",
    "            print('glove file does not exist, downloading from internet')\n",
    "            urllib.request.urlretrieve(url='http://nlp.stanford.edu/data/glove.6B.zip', filename=glove_zip,\n",
    "                                       reporthook=reporthook)\n",
    "\n",
    "        print('unzipping glove file')\n",
    "        zip_ref = zipfile.ZipFile(glove_zip, 'r')\n",
    "        zip_ref.extractall('data/glove-data')\n",
    "        zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporthook(block_num, block_size, total_size):\n",
    "    read_so_far = block_num * block_size\n",
    "    if total_size > 0:\n",
    "        percent = read_so_far * 1e2 / total_size\n",
    "        s = \"\\r%5.1f%% %*d / %d\" % (\n",
    "            percent, len(str(total_size)), read_so_far, total_size)\n",
    "        sys.stderr.write(s)\n",
    "        if read_so_far >= total_size:  # near the end\n",
    "            sys.stderr.write(\"\\n\")\n",
    "    else:  # total size is unknown\n",
    "        sys.stderr.write(\"read %d\\n\" % (read_so_far,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    '''\n",
    "    Function to read gloVe files\n",
    "        * return: dict of all words and their embedding vectors\n",
    "    '''\n",
    "    download_glove()\n",
    "    word2em = {}\n",
    "    file = open(GLOVE_MODEL, mode='rt', encoding='utf8')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        word2em[word] = embeds\n",
    "    file.close()\n",
    "    return word2em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_white_list(_word):\n",
    "    for char in _word:\n",
    "        if char in WHITELIST:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file data\n",
    "def load_data(path):\n",
    "    ''' Function to read training and testing files\n",
    "            *args:\n",
    "                path: file path as string \n",
    "            *return:\n",
    "                data: raw string text\n",
    "    '''\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    with open('models/preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CornellWordGloveChatBot(object):\n",
    "    model = None\n",
    "    encoder_model = None\n",
    "    decoder_model = None\n",
    "    target_word2idx = None\n",
    "    target_idx2word = None\n",
    "    max_decoder_seq_length = None\n",
    "    max_encoder_seq_length = None\n",
    "    num_decoder_tokens = None\n",
    "    word2em = None\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # load glove\n",
    "        self.word2em = load_glove()\n",
    "        \n",
    "        # load parameters\n",
    "        context, input_texts_word2em, target_texts, word2em, (self.target_word2idx,self.target_idx2word) = load_preprocess()\n",
    "        \n",
    "        self.max_encoder_seq_length = context['encoder_max_seq_length']\n",
    "        self.max_decoder_seq_length = context['decoder_max_seq_length']\n",
    "        self.num_decoder_tokens = context['num_decoder_tokens']\n",
    "\n",
    "        # encoder\n",
    "        encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
    "        encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name=\"encoder_lstm\")\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        # decoder\n",
    "        decoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='decoder_inputs')\n",
    "        decoder_lstm = LSTM(units=HIDDEN_UNITS, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # model inputs\n",
    "        self.model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "        # load saved model weights\n",
    "        self.model.load_weights('models/keras-glove-weights.h5')\n",
    "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "        decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "        \n",
    "    def reply(self, input_text):\n",
    "        '''\n",
    "        Function to generate a text response\n",
    "            *args:\n",
    "                input_text: message\n",
    "            *return:\n",
    "                generated text\n",
    "        '''\n",
    "        \n",
    "        input_seq = []\n",
    "        input_emb = []\n",
    "        #split senetence\n",
    "        for word in nltk.word_tokenize(input_text.lower()):\n",
    "            # check sentence characters\n",
    "            if not in_white_list(word):\n",
    "                continue\n",
    "            # create embedding vectors for input sentence\n",
    "            emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
    "            if word in self.word2em:\n",
    "                emb = self.word2em[word]\n",
    "            input_emb.append(emb)\n",
    "        input_seq.append(input_emb)\n",
    "        # padding senetence\n",
    "        input_seq = pad_sequences(input_seq, self.max_encoder_seq_length)\n",
    "        #predict\n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
    "        target_seq[0, 0, :] = self.word2em['start']\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'start' and sample_word != 'end':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'end' or target_text_len >= self.max_decoder_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
    "            if sample_word in self.word2em:\n",
    "                target_seq[0, 0, :] = self.word2em[sample_word]\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()\n",
    "\n",
    "    def test_run(self, sentance_lst):\n",
    "        '''\n",
    "        function to generate a response for a list of input text\n",
    "        '''\n",
    "        for sentance in sentance_lst:\n",
    "            print('  Input Message   : {}'.format(sentance))\n",
    "            print('  Response        : {}'.format(self.reply(sentance)))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'data/datasets/train.from'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test = load_data(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CornellWordGloveChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_run(source_test.split(\"\\n\")[98:103])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
