{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating a Text using the Chatbot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "import pickle\n",
    "import warnings\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from distutils.version import LooseVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Preprocessed Data and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them\n",
    "    \"\"\"\n",
    "    with open('models/preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "((source_int_text, target_int_text),\n",
    "(source_vocab_to_int, source_int_to_vocab),\n",
    "(target_vocab_to_int, target_int_to_vocab)) = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of comments: 13949\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size of comments:', len( source_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of replays: 13456\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size of replays:', len(target_vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    with open('models/params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence to Sequence\n",
    "To feed a sentence into the chatbot model, I first need to preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    ''' Function to convert a sentence to a sequence of ids\n",
    "            *args:\n",
    "                sentence: raw string\n",
    "                vocab_to_int: Dictionary to go from the words to an id\n",
    "            *return:\n",
    "                List of word ids\n",
    "    '''   \n",
    "    return [vocab_to_int.get(word, vocab_to_int.get('<UNK>')) for word in nltk.word_tokenize(sentence.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_replay(comment_sentence):\n",
    "    comment_sentence = sentence_to_seq(comment_sentence, source_vocab_to_int)\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "        loader.restore(sess, load_path)\n",
    "\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "        source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        replay_logits = sess.run(logits, {input_data: [comment_sentence]*batch_size,\n",
    "                                             target_sequence_length: [len(comment_sentence)*2]*batch_size,\n",
    "                                             source_sequence_length: [len(comment_sentence)]*batch_size,\n",
    "                                             keep_prob: 1.0})[0]\n",
    "\n",
    "    print('  Comment Words: {}'.format([source_int_to_vocab[i] for i in comment_sentence]))\n",
    "    print('  replay Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in replay_logits])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'data/train.from'\n",
    "target_path = 'data/test.to'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file data\n",
    "def load_data(path):\n",
    "    ''' Function to read training and testing files\n",
    "            *args:\n",
    "                path: file path as string \n",
    "            *return:\n",
    "                data: raw string text\n",
    "    '''\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test = load_data(source_path)\n",
    "target_test = load_data(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentance in source_test.split(\"\\n\")[100:105]:\n",
    "    #print(sentance)\n",
    "    predict_replay(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
