{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reddit data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the data for training, we need to convert the data to the format, that is required in order to train my RNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of the execution, I would have many files that will be used of the training and 2 files for testing:\n",
    "- Trining Files:\n",
    "    - train(serial).from (chatbot input)\n",
    "    - train(serial).to (chatbot output)\n",
    "- Test Files:\n",
    "    - test.from (chatbot input)\n",
    "    - test.to (chatbot output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(df):\n",
    "    \n",
    "    for row, index in zip(df[['parent', 'comment']].values , df.index.values):\n",
    "\n",
    "        # remove all coments which contains only special character\n",
    "        m_parent = re.match(r'^# \\*\\*\\[\\S+', row[0])\n",
    "        m_comment = re.match(r'^# \\*\\*\\[\\S+', row[1])\n",
    "        if m_parent or m_comment:\n",
    "            df.drop([index], inplace=True)\n",
    "            \n",
    "    # remove URLs\n",
    "    # remove special tags: '[tag name]'\n",
    "    df['parent'] = df['parent'].apply(lambda x: re.sub(r'(\\[[\\s \\w]+\\]\\()?http\\S+', '', x).strip())\n",
    "    df['comment'] = df['comment'].apply(lambda x: re.sub(r'(\\[[\\s \\w]+\\]\\()?http\\S+', '', x).strip())\n",
    "\n",
    "    # remove repeated ' newlinechar '\n",
    "    df['parent'] = df['parent'].apply(lambda x: re.sub(r'(\\snewlinechar\\s)+', 'newlinechar ', x).strip())\n",
    "    df['comment'] = df['comment'].apply(lambda x: re.sub(r'(\\snewlinechar\\s)+', 'newlinechar ', x).strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 rows completed so far\n",
      "200000 rows completed so far\n",
      "300000 rows completed so far\n",
      "400000 rows completed so far\n",
      "500000 rows completed so far\n",
      "600000 rows completed so far\n",
      "700000 rows completed so far\n",
      "800000 rows completed so far\n",
      "900000 rows completed so far\n"
     ]
    }
   ],
   "source": [
    "timeframes = ['2018-06']\n",
    "\n",
    "# if i have more than one database (more than one month)\n",
    "for timeframe in timeframes:\n",
    "    \n",
    "    # establish a connection\n",
    "    connection = sqlite3.connect('../data/raw_data/reddit/{}.db'.format(timeframe))\n",
    "    c = connection.cursor()\n",
    "    \n",
    "    # limit is the size of chunk that we're going to pull at a time from the database\n",
    "    limit = 50000\n",
    "    #time stamp\n",
    "    last_unix = 0\n",
    "    cur_length = limit\n",
    "    counter = 0\n",
    "    #when we're done building testing data.\n",
    "    test_done = False\n",
    "    \n",
    "    # help in naming training files\n",
    "    train_size = 0\n",
    "    file_name = 1\n",
    "\n",
    "    #So long as the cur_length is the same as our limit, we've still got more pulling to do. \n",
    "    while cur_length == limit:\n",
    "\n",
    "        # fetch data and save it in dataframe\n",
    "        df = pd.read_sql(\"SELECT * FROM parent_reply WHERE unix > {} and parent <> 'None' and score > 0 ORDER BY unix ASC LIMIT {}\".format(last_unix,limit),connection)\n",
    "        \n",
    "        # last fetched unix\n",
    "        last_unix = df.tail(1)['unix'].values[0]\n",
    "        \n",
    "        # length of our dataframe\n",
    "        cur_length = len(df)\n",
    "\n",
    "        # clean text\n",
    "        df = clean_comments(df)\n",
    "        \n",
    "        # need to create sperated files for test and train\n",
    "        \n",
    "        # test files\n",
    "        if not test_done:\n",
    "            # create a file for all parent text only\n",
    "            with open('../data/datasets/reddit/test.from','a', encoding='utf8') as f:\n",
    "                for content in df['parent'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "                    \n",
    "            # create a file for all comment text only\n",
    "            with open('../data/datasets/reddit/test.to','a', encoding='utf8') as f:\n",
    "                for content in df['comment'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "\n",
    "            test_done = True\n",
    "\n",
    "        # train files\n",
    "        else:\n",
    "            train_size += len(df)\n",
    "            \n",
    "            # create a file for all parent text only\n",
    "            with open('../data/datasets/reddit/'+'train'+str(file_name)+'.from','a', encoding='utf8') as f:\n",
    "                for content in df['parent'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "                    \n",
    "            # create a file for all comment text only\n",
    "            with open('../data/datasets/reddit/'+'train'+str(file_name)+'.to','a', encoding='utf8') as f:\n",
    "                for content in df['comment'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "            \n",
    "            # create files with 1,000,000 rows\n",
    "            if train_size >= 1000000:\n",
    "                break\n",
    "            elif train_size >= file_name * 100000:\n",
    "                file_name += 1\n",
    "                print(train_size,'rows completed so far')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
