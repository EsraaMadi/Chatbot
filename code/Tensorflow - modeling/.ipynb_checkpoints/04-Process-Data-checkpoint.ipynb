{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Tensorflow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with text data is to pre-process it. I cannot go straight from raw text to fitting a machine learning model. I must clean the text first, which means splitting it into words and handling punctuation and case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paramters\n",
    "\n",
    "# training file path (movies) ,(reddit)\n",
    "source_path = ['data/train.from', 'data/datasets/reddit/train1.from']\n",
    "target_path = ['data/train.to', 'data/datasets/reddit/train1.to']\n",
    "\n",
    "# special codes\n",
    "CODES = ['<PAD>', '<EOS>', '<UNK>', '<GO>']\n",
    "\n",
    "MAX_TARGET_SEQ_LENGTH = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file data\n",
    "def load_data(path):\n",
    "    ''' Function to read training and testing files\n",
    "            *args:\n",
    "                path: file path as string \n",
    "            *return:\n",
    "                data: raw string text\n",
    "    '''\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_accurance(text):\n",
    "    ''' \n",
    "    Function to split big text to words, then give each word a unique number as code\n",
    "        *args:\n",
    "            text: raw string\n",
    "        *return:\n",
    "            vocab_to_int: dictionary {word:int_code}\n",
    "            int_to_vocab: dictionary {int_code:word} \n",
    "    '''\n",
    "    vocab = {}\n",
    "    for sentance in text.split(\"\\n\"):\n",
    "        sentance = [w for w in nltk.word_tokenize(sentance)]\n",
    "        if len(sentance) <= MAX_TARGET_SEQ_LENGTH:\n",
    "            #sentance = sentance[0:MAX_TARGET_SEQ_LENGTH]\n",
    "            for word in sentance:\n",
    "                if vocab.get(word) == None:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] = vocab.get(word)+1\n",
    "\n",
    "    vocab_to_int = {'<PAD>':0, '<EOS>':1, '<UNK>':2, '<GO>':3}\n",
    "    i = 4\n",
    "    for key, item in (vocab.items()):\n",
    "        if item > 1:\n",
    "            vocab_to_int[key] = i\n",
    "            i += 1\n",
    "            \n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(source_text, target_text, target_vocab_to_int, source_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Function to covert string (words) to index\n",
    "        *args:\n",
    "            source_text: raw string text for comments\n",
    "            target_text: raw string text for replies\n",
    "            vocab_to_int: lookup tables\n",
    "        *return:\n",
    "            source_text_id: A list of lists source_id_text converted\n",
    "            target_text_id: A list of lists target_id_text converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences \n",
    "    for i in range(len(source_sentences)): # of sentences in source&target is the same)\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        if len(source_tokens) <= MAX_TARGET_SEQ_LENGTH and len(target_tokens) <= MAX_TARGET_SEQ_LENGTH:\n",
    "            \n",
    "            # empty list of converted words to index in the chosen sentence\n",
    "            source_token_id = []\n",
    "            target_token_id = []\n",
    "\n",
    "            for index, token in enumerate(source_tokens):\n",
    "                if (token != \"\"):\n",
    "                    source_token_id.append(source_vocab_to_int.get(token,source_vocab_to_int['<UNK>']))\n",
    "\n",
    "            for index, token in enumerate(target_tokens):\n",
    "                if (token != \"\"):\n",
    "                    target_token_id.append(target_vocab_to_int.get(token,target_vocab_to_int['<UNK>']))\n",
    "\n",
    "            # put <EOS> token at the end of the chosen target sentence\n",
    "            # this token suggests when to stop creating a sequence\n",
    "            target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "\n",
    "            # add each converted sentences in the final list\n",
    "            source_text_id.append(source_token_id)\n",
    "            target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(source_path, target_path):\n",
    "    '''\n",
    "    Function to preprocess data files and save the following\n",
    "        1- source_text after encoding to index (comment) \n",
    "        2- target_text after encoding to index (replay)\n",
    "        3- vocab_to_int dict\n",
    "        4- int_to_vocab dict\n",
    "        *args:\n",
    "            source_path: list of comment files path as string \n",
    "            target_path: list of replay files path as string \n",
    "    '''\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original training data (comment, replay) Movies\n",
    "    mov_source_text = load_data(source_path[0]).lower()\n",
    "    mov_target_text = load_data(target_path[0]).lower()\n",
    "    \n",
    "#     # load original training data (comment, replay) Reddit\n",
    "#     red_source_text = load_data(source_path[1]).lower()\n",
    "#     red_target_text = load_data(target_path[1]).lower()\n",
    "\n",
    "\n",
    "    # create lookup dict for (comment, replay) data\n",
    "    source_vocab_to_int, source_int_to_vocab = vocab_accurance(mov_source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab =  vocab_accurance(mov_target_text)\n",
    "    \n",
    "    \n",
    "    # create list of sentences whose words are represented in index\n",
    "    mov_source_text, mov_target_text = text_to_index(mov_source_text,\n",
    "                                                     mov_target_text,\n",
    "                                                     target_vocab_to_int,\n",
    "                                                     source_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (mov_source_text, mov_target_text),\n",
    "        (source_vocab_to_int, source_int_to_vocab),\n",
    "        (target_vocab_to_int, target_int_to_vocab)), open('models/preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process source and target files\n",
    "preprocess_and_save_data(source_path, target_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
